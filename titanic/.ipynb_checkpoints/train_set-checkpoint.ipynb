{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for training\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan(array):\n",
    "    nan_in_array = np.array([np.isnan(t) for t in array])\n",
    "    count_of_nan = np.count_nonzero(nan_in_array == 1)# to check the number of null entries\n",
    "    return count_of_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a better way to get the number of nulls for all data types:\n",
    "# a better check nan function that works with any dtype:\n",
    "def check_nan2(array):\n",
    "    unique, counts = np.unique(array,return_counts=True)\n",
    "    dict_ = dict(zip(unique, counts))\n",
    "    try :\n",
    "        return dict_[\"nan\"]\n",
    "    except: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age = np.array(train.loc[:, \"Age\"]) #1-80\n",
    "target = np.array(train.loc[:,\"Survived\"]) # 0/1\n",
    "\n",
    "count_of_nan = check_nan(age)\n",
    "# ok a lot of nan :(  \n",
    "#lets replace them with the median age:\n",
    "values_without_nan = np.array([ t for t in age if not np.isnan(t)])\n",
    "\n",
    "median = np.median(values_without_nan)\n",
    "\n",
    "p = lambda s : median if np.isnan(s) else s\n",
    "\n",
    "age_new= np.array([p(t) for t in age])         \n",
    "count_of_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_new = age_new/80 # 1-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f4da412b2d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "survived_age = np.array([age[i] for i in range(len(age)) if target[i]])\n",
    "plt.hist([age,survived_age],bins=10, label=[\"total\",\"survived\"])\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(577, 314)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = lambda a: 1 if a.lower() == \"male\" else 0\n",
    "gender = np.array([x(i) for i in train.loc[:,\"Sex\"]])#0/1\n",
    "\n",
    "# finding the number of males and females for fun \n",
    "number_of_males = np.count_nonzero(gender ==1)\n",
    "number_of_females = np.count_nonzero(gender ==0) #yeah the \"nonzero\" is  ajoke XD\n",
    "number_of_males, number_of_females "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0052499999999999995, 1.0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_new.min(), age_new.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived\n",
      "Blahblahblah\n",
      "Pclass\n",
      "Sex\n",
      "Age\n",
      "SibSp\n",
      "Parch\n",
      "Ticket\n",
      "Fare\n",
      "Cabin\n",
      "Embarked\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Survived': array([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1,\n",
       "        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]),\n",
       " 'Pclass': array([3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3, 2, 2,\n",
       "        3, 1, 3, 3, 3, 1, 3, 3, 1, 1, 3, 2, 1, 1, 3, 3, 3, 3, 3, 2, 3, 2,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 3, 2, 3, 3, 1, 1, 3, 1, 3,\n",
       "        2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 1, 2, 3, 3, 3,\n",
       "        1, 3, 3, 3, 1, 3, 3, 3, 1, 1, 2, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3,\n",
       "        1, 3, 3, 3, 3, 3, 3, 2, 1, 3, 2, 3, 2, 2, 1, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 2, 2, 2, 1, 1, 3, 1, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 1, 3, 3,\n",
       "        3, 1, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 1, 3, 1, 3, 1, 3, 3, 3, 1, 3,\n",
       "        3, 1, 2, 3, 3, 2, 3, 2, 3, 1, 3, 1, 3, 3, 2, 2, 3, 2, 1, 1, 3, 3,\n",
       "        3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 2, 3, 2, 3, 1, 3, 2, 1, 2,\n",
       "        3, 2, 3, 3, 1, 3, 2, 3, 2, 3, 1, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 3,\n",
       "        2, 3, 3, 1, 3, 2, 1, 2, 3, 3, 1, 3, 3, 3, 1, 1, 1, 2, 3, 3, 1, 1,\n",
       "        3, 2, 3, 3, 1, 1, 1, 3, 2, 1, 3, 1, 3, 2, 3, 3, 3, 3, 3, 3, 1, 3,\n",
       "        3, 3, 2, 3, 1, 1, 2, 3, 3, 1, 3, 1, 1, 1, 3, 3, 3, 2, 3, 1, 1, 1,\n",
       "        2, 1, 1, 1, 2, 3, 2, 3, 2, 2, 1, 1, 3, 3, 2, 2, 3, 1, 3, 2, 3, 1,\n",
       "        3, 1, 1, 3, 1, 3, 1, 1, 3, 1, 2, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 1,\n",
       "        3, 3, 3, 3, 1, 2, 3, 3, 3, 2, 3, 3, 3, 3, 1, 3, 3, 1, 1, 3, 3, 1,\n",
       "        3, 1, 3, 1, 3, 3, 1, 3, 3, 1, 3, 2, 3, 2, 3, 2, 1, 3, 3, 1, 3, 3,\n",
       "        3, 2, 2, 2, 3, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 1, 2, 3, 3, 2, 2,\n",
       "        2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 1, 3, 2, 3, 1, 1, 3, 2, 1, 2,\n",
       "        2, 3, 3, 2, 3, 1, 2, 1, 3, 1, 2, 3, 1, 1, 3, 3, 1, 1, 2, 3, 1, 3,\n",
       "        1, 2, 3, 3, 2, 1, 3, 3, 3, 3, 2, 2, 3, 1, 2, 3, 3, 3, 3, 2, 3, 3,\n",
       "        1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 1, 1,\n",
       "        2, 1, 3, 3, 3, 3, 1, 1, 3, 1, 2, 3, 2, 3, 1, 3, 3, 1, 3, 3, 2, 1,\n",
       "        3, 2, 2, 3, 3, 3, 3, 2, 1, 1, 3, 1, 1, 3, 3, 2, 1, 1, 2, 2, 3, 2,\n",
       "        1, 2, 3, 3, 3, 1, 1, 1, 1, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 1,\n",
       "        1, 3, 3, 3, 2, 1, 3, 3, 2, 1, 2, 1, 3, 1, 2, 1, 3, 3, 3, 1, 3, 3,\n",
       "        2, 3, 2, 3, 3, 1, 2, 3, 1, 3, 1, 3, 3, 1, 2, 1, 3, 3, 3, 3, 3, 2,\n",
       "        3, 3, 2, 2, 3, 1, 3, 3, 3, 1, 2, 1, 3, 3, 1, 3, 1, 1, 3, 2, 3, 2,\n",
       "        3, 3, 3, 1, 3, 3, 3, 1, 3, 1, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 2, 1,\n",
       "        1, 3, 1, 3, 3, 2, 2, 3, 3, 1, 2, 1, 2, 2, 2, 3, 3, 3, 3, 1, 3, 1,\n",
       "        3, 3, 2, 2, 3, 3, 3, 1, 1, 3, 3, 3, 1, 2, 3, 3, 1, 3, 1, 1, 3, 3,\n",
       "        3, 2, 2, 1, 1, 3, 1, 1, 1, 3, 2, 3, 1, 2, 3, 3, 2, 3, 2, 2, 1, 3,\n",
       "        2, 3, 2, 3, 1, 3, 2, 2, 2, 3, 3, 1, 3, 3, 1, 1, 1, 3, 3, 1, 3, 2,\n",
       "        1, 3, 2, 3, 3, 3, 2, 2, 3, 2, 3, 1, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3,\n",
       "        3, 3, 2, 3, 2, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 1, 3, 2,\n",
       "        3, 1, 3, 2, 1, 3, 3, 3, 2, 2, 1, 3, 3, 3, 1, 3, 2, 1, 3, 3, 2, 3,\n",
       "        3, 1, 3, 2, 3, 3, 1, 3, 1, 3, 3, 3, 3, 2, 3, 1, 3, 2, 3, 3, 3, 1,\n",
       "        3, 3, 3, 1, 3, 2, 1, 3, 3, 3, 3, 3, 2, 1, 3, 3, 3, 1, 2, 3, 1, 1,\n",
       "        3, 3, 3, 2, 1, 3, 2, 2, 2, 1, 3, 3, 3, 1, 1, 3, 2, 3, 3, 3, 3, 1,\n",
       "        2, 3, 3, 2, 3, 3, 2, 1, 3, 1, 3]),\n",
       " 'Sex': array(['male', 'female', 'female', 'female', 'male', 'male', 'male',\n",
       "        'male', 'female', 'female', 'female', 'female', 'male', 'male',\n",
       "        'female', 'female', 'male', 'male', 'female', 'female', 'male',\n",
       "        'male', 'female', 'male', 'female', 'female', 'male', 'male',\n",
       "        'female', 'male', 'male', 'female', 'female', 'male', 'male',\n",
       "        'male', 'male', 'male', 'female', 'female', 'female', 'female',\n",
       "        'male', 'female', 'female', 'male', 'male', 'female', 'male',\n",
       "        'female', 'male', 'male', 'female', 'female', 'male', 'male',\n",
       "        'female', 'male', 'female', 'male', 'male', 'female', 'male',\n",
       "        'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male',\n",
       "        'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male',\n",
       "        'female', 'male', 'male', 'female', 'male', 'female', 'female',\n",
       "        'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male',\n",
       "        'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male',\n",
       "        'male', 'male', 'male', 'male', 'female', 'male', 'male', 'female',\n",
       "        'male', 'female', 'male', 'female', 'female', 'male', 'male',\n",
       "        'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male',\n",
       "        'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female',\n",
       "        'female', 'male', 'male', 'female', 'male', 'male', 'male',\n",
       "        'female', 'female', 'female', 'male', 'male', 'male', 'male',\n",
       "        'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male',\n",
       "        'male', 'female', 'male', 'male', 'male', 'male', 'female', 'male',\n",
       "        'male', 'male', 'male', 'female', 'female', 'male', 'male', 'male',\n",
       "        'male', 'female', 'male', 'male', 'male', 'male', 'female', 'male',\n",
       "        'male', 'female', 'male', 'male', 'male', 'female', 'male',\n",
       "        'female', 'male', 'male', 'male', 'female', 'male', 'female',\n",
       "        'male', 'female', 'female', 'male', 'male', 'female', 'female',\n",
       "        'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male',\n",
       "        'female', 'male', 'male', 'female', 'male', 'male', 'male',\n",
       "        'female', 'female', 'male', 'female', 'male', 'male', 'male',\n",
       "        'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female',\n",
       "        'female', 'male', 'male', 'female', 'male', 'female', 'male',\n",
       "        'female', 'male', 'male', 'female', 'female', 'male', 'male',\n",
       "        'male', 'male', 'female', 'female', 'male', 'male', 'male',\n",
       "        'female', 'male', 'male', 'female', 'female', 'female', 'female',\n",
       "        'female', 'female', 'male', 'male', 'male', 'male', 'female',\n",
       "        'male', 'male', 'male', 'female', 'female', 'male', 'male',\n",
       "        'female', 'male', 'female', 'female', 'female', 'male', 'male',\n",
       "        'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male',\n",
       "        'male', 'male', 'female', 'female', 'female', 'male', 'female',\n",
       "        'male', 'male', 'male', 'female', 'male', 'female', 'female',\n",
       "        'male', 'male', 'female', 'male', 'male', 'female', 'female',\n",
       "        'male', 'female', 'female', 'female', 'female', 'male', 'male',\n",
       "        'female', 'female', 'male', 'female', 'female', 'male', 'male',\n",
       "        'female', 'female', 'male', 'female', 'male', 'female', 'female',\n",
       "        'female', 'female', 'male', 'male', 'male', 'female', 'male',\n",
       "        'male', 'female', 'male', 'male', 'male', 'female', 'male', 'male',\n",
       "        'male', 'female', 'female', 'female', 'male', 'male', 'male',\n",
       "        'male', 'male', 'male', 'male', 'male', 'female', 'female',\n",
       "        'female', 'female', 'male', 'male', 'female', 'male', 'male',\n",
       "        'male', 'female', 'female', 'female', 'female', 'male', 'male',\n",
       "        'male', 'male', 'female', 'female', 'female', 'male', 'male',\n",
       "        'male', 'female', 'female', 'male', 'female', 'male', 'male',\n",
       "        'male', 'female', 'male', 'female', 'male', 'male', 'male',\n",
       "        'female', 'female', 'male', 'female', 'male', 'male', 'female',\n",
       "        'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male',\n",
       "        'male', 'female', 'male', 'male', 'female', 'male', 'male',\n",
       "        'female', 'female', 'female', 'male', 'female', 'male', 'male',\n",
       "        'male', 'female', 'male', 'male', 'female', 'female', 'male',\n",
       "        'male', 'male', 'female', 'female', 'male', 'male', 'female',\n",
       "        'female', 'female', 'male', 'male', 'female', 'male', 'male',\n",
       "        'female', 'male', 'male', 'female', 'male', 'female', 'male',\n",
       "        'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female',\n",
       "        'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male',\n",
       "        'male', 'male', 'male', 'female', 'male', 'male', 'female',\n",
       "        'female', 'female', 'male', 'male', 'male', 'male', 'female',\n",
       "        'male', 'male', 'male', 'female', 'male', 'female', 'female',\n",
       "        'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male',\n",
       "        'male', 'female', 'male', 'female', 'male', 'male', 'female',\n",
       "        'female', 'female', 'female', 'male', 'female', 'male', 'male',\n",
       "        'male', 'male', 'male', 'male', 'female', 'male', 'male', 'female',\n",
       "        'male', 'female', 'male', 'female', 'male', 'male', 'female',\n",
       "        'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male',\n",
       "        'male', 'female', 'female', 'female', 'male', 'female', 'male',\n",
       "        'female', 'female', 'female', 'female', 'male', 'male', 'male',\n",
       "        'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male',\n",
       "        'female', 'male', 'female', 'male', 'female', 'female', 'male',\n",
       "        'male', 'male', 'male', 'female', 'male', 'male', 'female', 'male',\n",
       "        'male', 'male', 'female', 'male', 'female', 'male', 'male',\n",
       "        'female', 'female', 'female', 'male', 'female', 'female', 'male',\n",
       "        'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male',\n",
       "        'female', 'male', 'female', 'male', 'male', 'female', 'male',\n",
       "        'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male',\n",
       "        'male', 'male', 'female', 'female', 'female', 'male', 'female',\n",
       "        'male', 'male', 'female', 'male', 'female', 'female', 'male',\n",
       "        'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female',\n",
       "        'male', 'male', 'male', 'male', 'male', 'male', 'female', 'female',\n",
       "        'male', 'male', 'female', 'male', 'male', 'female', 'female',\n",
       "        'male', 'female', 'male', 'male', 'male', 'male', 'female', 'male',\n",
       "        'female', 'male', 'female', 'female', 'male', 'male', 'female',\n",
       "        'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male',\n",
       "        'male', 'male', 'male', 'female', 'female', 'male', 'male', 'male',\n",
       "        'male', 'male', 'male', 'female', 'female', 'male', 'female',\n",
       "        'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male',\n",
       "        'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male',\n",
       "        'female', 'male', 'male', 'female', 'male', 'female', 'male',\n",
       "        'male', 'male', 'female', 'male', 'female', 'male', 'female',\n",
       "        'male', 'male', 'male', 'male', 'male', 'female', 'female', 'male',\n",
       "        'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female',\n",
       "        'female', 'male', 'female', 'female', 'male', 'male', 'male',\n",
       "        'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male',\n",
       "        'female', 'male', 'male', 'male', 'male', 'female', 'male', 'male',\n",
       "        'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male',\n",
       "        'male', 'female', 'male', 'male', 'male', 'female', 'male',\n",
       "        'female', 'male', 'female', 'male', 'male', 'male', 'male',\n",
       "        'female', 'male', 'female', 'male', 'male', 'female', 'male',\n",
       "        'female', 'female', 'female', 'male', 'male', 'male', 'male',\n",
       "        'female', 'male', 'male', 'male', 'male', 'male', 'female', 'male',\n",
       "        'male', 'male', 'female', 'female', 'male', 'female', 'male',\n",
       "        'female', 'male', 'male', 'male', 'male', 'male', 'female', 'male',\n",
       "        'female', 'male', 'male', 'male', 'female', 'male', 'male',\n",
       "        'female', 'male', 'male', 'male', 'female', 'male', 'male',\n",
       "        'female', 'male', 'male', 'male', 'male', 'male', 'female',\n",
       "        'female', 'male', 'male', 'male', 'male', 'female', 'male', 'male',\n",
       "        'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male',\n",
       "        'male', 'male', 'male', 'female', 'male', 'male', 'female',\n",
       "        'female', 'female', 'female', 'female', 'male', 'female', 'male',\n",
       "        'male', 'male', 'female', 'female', 'male', 'female', 'female',\n",
       "        'male', 'male', 'male', 'male', 'female', 'male', 'male', 'female',\n",
       "        'female', 'male', 'male', 'male', 'female', 'female', 'male',\n",
       "        'female', 'male', 'male', 'female', 'male', 'female', 'female',\n",
       "        'male', 'male'], dtype='<U6'),\n",
       " 'Age': array([22.  , 38.  , 26.  , 35.  , 35.  ,   nan, 54.  ,  2.  , 27.  ,\n",
       "        14.  ,  4.  , 58.  , 20.  , 39.  , 14.  , 55.  ,  2.  ,   nan,\n",
       "        31.  ,   nan, 35.  , 34.  , 15.  , 28.  ,  8.  , 38.  ,   nan,\n",
       "        19.  ,   nan,   nan, 40.  ,   nan,   nan, 66.  , 28.  , 42.  ,\n",
       "          nan, 21.  , 18.  , 14.  , 40.  , 27.  ,   nan,  3.  , 19.  ,\n",
       "          nan,   nan,   nan,   nan, 18.  ,  7.  , 21.  , 49.  , 29.  ,\n",
       "        65.  ,   nan, 21.  , 28.5 ,  5.  , 11.  , 22.  , 38.  , 45.  ,\n",
       "         4.  ,   nan,   nan, 29.  , 19.  , 17.  , 26.  , 32.  , 16.  ,\n",
       "        21.  , 26.  , 32.  , 25.  ,   nan,   nan,  0.83, 30.  , 22.  ,\n",
       "        29.  ,   nan, 28.  , 17.  , 33.  , 16.  ,   nan, 23.  , 24.  ,\n",
       "        29.  , 20.  , 46.  , 26.  , 59.  ,   nan, 71.  , 23.  , 34.  ,\n",
       "        34.  , 28.  ,   nan, 21.  , 33.  , 37.  , 28.  , 21.  ,   nan,\n",
       "        38.  ,   nan, 47.  , 14.5 , 22.  , 20.  , 17.  , 21.  , 70.5 ,\n",
       "        29.  , 24.  ,  2.  , 21.  ,   nan, 32.5 , 32.5 , 54.  , 12.  ,\n",
       "          nan, 24.  ,   nan, 45.  , 33.  , 20.  , 47.  , 29.  , 25.  ,\n",
       "        23.  , 19.  , 37.  , 16.  , 24.  ,   nan, 22.  , 24.  , 19.  ,\n",
       "        18.  , 19.  , 27.  ,  9.  , 36.5 , 42.  , 51.  , 22.  , 55.5 ,\n",
       "        40.5 ,   nan, 51.  , 16.  , 30.  ,   nan,   nan, 44.  , 40.  ,\n",
       "        26.  , 17.  ,  1.  ,  9.  ,   nan, 45.  ,   nan, 28.  , 61.  ,\n",
       "         4.  ,  1.  , 21.  , 56.  , 18.  ,   nan, 50.  , 30.  , 36.  ,\n",
       "          nan,   nan,  9.  ,  1.  ,  4.  ,   nan,   nan, 45.  , 40.  ,\n",
       "        36.  , 32.  , 19.  , 19.  ,  3.  , 44.  , 58.  ,   nan, 42.  ,\n",
       "          nan, 24.  , 28.  ,   nan, 34.  , 45.5 , 18.  ,  2.  , 32.  ,\n",
       "        26.  , 16.  , 40.  , 24.  , 35.  , 22.  , 30.  ,   nan, 31.  ,\n",
       "        27.  , 42.  , 32.  , 30.  , 16.  , 27.  , 51.  ,   nan, 38.  ,\n",
       "        22.  , 19.  , 20.5 , 18.  ,   nan, 35.  , 29.  , 59.  ,  5.  ,\n",
       "        24.  ,   nan, 44.  ,  8.  , 19.  , 33.  ,   nan,   nan, 29.  ,\n",
       "        22.  , 30.  , 44.  , 25.  , 24.  , 37.  , 54.  ,   nan, 29.  ,\n",
       "        62.  , 30.  , 41.  , 29.  ,   nan, 30.  , 35.  , 50.  ,   nan,\n",
       "         3.  , 52.  , 40.  ,   nan, 36.  , 16.  , 25.  , 58.  , 35.  ,\n",
       "          nan, 25.  , 41.  , 37.  ,   nan, 63.  , 45.  ,   nan,  7.  ,\n",
       "        35.  , 65.  , 28.  , 16.  , 19.  ,   nan, 33.  , 30.  , 22.  ,\n",
       "        42.  , 22.  , 26.  , 19.  , 36.  , 24.  , 24.  ,   nan, 23.5 ,\n",
       "         2.  ,   nan, 50.  ,   nan,   nan, 19.  ,   nan,   nan,  0.92,\n",
       "          nan, 17.  , 30.  , 30.  , 24.  , 18.  , 26.  , 28.  , 43.  ,\n",
       "        26.  , 24.  , 54.  , 31.  , 40.  , 22.  , 27.  , 30.  , 22.  ,\n",
       "          nan, 36.  , 61.  , 36.  , 31.  , 16.  ,   nan, 45.5 , 38.  ,\n",
       "        16.  ,   nan,   nan, 29.  , 41.  , 45.  , 45.  ,  2.  , 24.  ,\n",
       "        28.  , 25.  , 36.  , 24.  , 40.  ,   nan,  3.  , 42.  , 23.  ,\n",
       "          nan, 15.  , 25.  ,   nan, 28.  , 22.  , 38.  ,   nan,   nan,\n",
       "        40.  , 29.  , 45.  , 35.  ,   nan, 30.  , 60.  ,   nan,   nan,\n",
       "        24.  , 25.  , 18.  , 19.  , 22.  ,  3.  ,   nan, 22.  , 27.  ,\n",
       "        20.  , 19.  , 42.  ,  1.  , 32.  , 35.  ,   nan, 18.  ,  1.  ,\n",
       "        36.  ,   nan, 17.  , 36.  , 21.  , 28.  , 23.  , 24.  , 22.  ,\n",
       "        31.  , 46.  , 23.  , 28.  , 39.  , 26.  , 21.  , 28.  , 20.  ,\n",
       "        34.  , 51.  ,  3.  , 21.  ,   nan,   nan,   nan, 33.  ,   nan,\n",
       "        44.  ,   nan, 34.  , 18.  , 30.  , 10.  ,   nan, 21.  , 29.  ,\n",
       "        28.  , 18.  ,   nan, 28.  , 19.  ,   nan, 32.  , 28.  ,   nan,\n",
       "        42.  , 17.  , 50.  , 14.  , 21.  , 24.  , 64.  , 31.  , 45.  ,\n",
       "        20.  , 25.  , 28.  ,   nan,  4.  , 13.  , 34.  ,  5.  , 52.  ,\n",
       "        36.  ,   nan, 30.  , 49.  ,   nan, 29.  , 65.  ,   nan, 50.  ,\n",
       "          nan, 48.  , 34.  , 47.  , 48.  ,   nan, 38.  ,   nan, 56.  ,\n",
       "          nan,  0.75,   nan, 38.  , 33.  , 23.  , 22.  ,   nan, 34.  ,\n",
       "        29.  , 22.  ,  2.  ,  9.  ,   nan, 50.  , 63.  , 25.  ,   nan,\n",
       "        35.  , 58.  , 30.  ,  9.  ,   nan, 21.  , 55.  , 71.  , 21.  ,\n",
       "          nan, 54.  ,   nan, 25.  , 24.  , 17.  , 21.  ,   nan, 37.  ,\n",
       "        16.  , 18.  , 33.  ,   nan, 28.  , 26.  , 29.  ,   nan, 36.  ,\n",
       "        54.  , 24.  , 47.  , 34.  ,   nan, 36.  , 32.  , 30.  , 22.  ,\n",
       "          nan, 44.  ,   nan, 40.5 , 50.  ,   nan, 39.  , 23.  ,  2.  ,\n",
       "          nan, 17.  ,   nan, 30.  ,  7.  , 45.  , 30.  ,   nan, 22.  ,\n",
       "        36.  ,  9.  , 11.  , 32.  , 50.  , 64.  , 19.  ,   nan, 33.  ,\n",
       "         8.  , 17.  , 27.  ,   nan, 22.  , 22.  , 62.  , 48.  ,   nan,\n",
       "        39.  , 36.  ,   nan, 40.  , 28.  ,   nan,   nan, 24.  , 19.  ,\n",
       "        29.  ,   nan, 32.  , 62.  , 53.  , 36.  ,   nan, 16.  , 19.  ,\n",
       "        34.  , 39.  ,   nan, 32.  , 25.  , 39.  , 54.  , 36.  ,   nan,\n",
       "        18.  , 47.  , 60.  , 22.  ,   nan, 35.  , 52.  , 47.  ,   nan,\n",
       "        37.  , 36.  ,   nan, 49.  ,   nan, 49.  , 24.  ,   nan,   nan,\n",
       "        44.  , 35.  , 36.  , 30.  , 27.  , 22.  , 40.  , 39.  ,   nan,\n",
       "          nan,   nan, 35.  , 24.  , 34.  , 26.  ,  4.  , 26.  , 27.  ,\n",
       "        42.  , 20.  , 21.  , 21.  , 61.  , 57.  , 21.  , 26.  ,   nan,\n",
       "        80.  , 51.  , 32.  ,   nan,  9.  , 28.  , 32.  , 31.  , 41.  ,\n",
       "          nan, 20.  , 24.  ,  2.  ,   nan,  0.75, 48.  , 19.  , 56.  ,\n",
       "          nan, 23.  ,   nan, 18.  , 21.  ,   nan, 18.  , 24.  ,   nan,\n",
       "        32.  , 23.  , 58.  , 50.  , 40.  , 47.  , 36.  , 20.  , 32.  ,\n",
       "        25.  ,   nan, 43.  ,   nan, 40.  , 31.  , 70.  , 31.  ,   nan,\n",
       "        18.  , 24.5 , 18.  , 43.  , 36.  ,   nan, 27.  , 20.  , 14.  ,\n",
       "        60.  , 25.  , 14.  , 19.  , 18.  , 15.  , 31.  ,  4.  ,   nan,\n",
       "        25.  , 60.  , 52.  , 44.  ,   nan, 49.  , 42.  , 18.  , 35.  ,\n",
       "        18.  , 25.  , 26.  , 39.  , 45.  , 42.  , 22.  ,   nan, 24.  ,\n",
       "          nan, 48.  , 29.  , 52.  , 19.  , 38.  , 27.  ,   nan, 33.  ,\n",
       "         6.  , 17.  , 34.  , 50.  , 27.  , 20.  , 30.  ,   nan, 25.  ,\n",
       "        25.  , 29.  , 11.  ,   nan, 23.  , 23.  , 28.5 , 48.  , 35.  ,\n",
       "          nan,   nan,   nan, 36.  , 21.  , 24.  , 31.  , 70.  , 16.  ,\n",
       "        30.  , 19.  , 31.  ,  4.  ,  6.  , 33.  , 23.  , 48.  ,  0.67,\n",
       "        28.  , 18.  , 34.  , 33.  ,   nan, 41.  , 20.  , 36.  , 16.  ,\n",
       "        51.  ,   nan, 30.5 ,   nan, 32.  , 24.  , 48.  , 57.  ,   nan,\n",
       "        54.  , 18.  ,   nan,  5.  ,   nan, 43.  , 13.  , 17.  , 29.  ,\n",
       "          nan, 25.  , 25.  , 18.  ,  8.  ,  1.  , 46.  ,   nan, 16.  ,\n",
       "          nan,   nan, 25.  , 39.  , 49.  , 31.  , 30.  , 30.  , 34.  ,\n",
       "        31.  , 11.  ,  0.42, 27.  , 31.  , 39.  , 18.  , 39.  , 33.  ,\n",
       "        26.  , 39.  , 35.  ,  6.  , 30.5 ,   nan, 23.  , 31.  , 43.  ,\n",
       "        10.  , 52.  , 27.  , 38.  , 27.  ,  2.  ,   nan,   nan,  1.  ,\n",
       "          nan, 62.  , 15.  ,  0.83,   nan, 23.  , 18.  , 39.  , 21.  ,\n",
       "          nan, 32.  ,   nan, 20.  , 16.  , 30.  , 34.5 , 17.  , 42.  ,\n",
       "          nan, 35.  , 28.  ,   nan,  4.  , 74.  ,  9.  , 16.  , 44.  ,\n",
       "        18.  , 45.  , 51.  , 24.  ,   nan, 41.  , 21.  , 48.  ,   nan,\n",
       "        24.  , 42.  , 27.  , 31.  ,   nan,  4.  , 26.  , 47.  , 33.  ,\n",
       "        47.  , 28.  , 15.  , 20.  , 19.  ,   nan, 56.  , 25.  , 33.  ,\n",
       "        22.  , 28.  , 25.  , 39.  , 27.  , 19.  ,   nan, 26.  , 32.  ]),\n",
       " 'SibSp': array([1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0, 0, 0,\n",
       "        0, 0, 3, 1, 0, 3, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1,\n",
       "        0, 0, 1, 0, 2, 1, 4, 0, 1, 1, 0, 0, 0, 0, 1, 5, 0, 0, 1, 3, 0, 1,\n",
       "        0, 0, 4, 2, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 1, 0,\n",
       "        3, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 1, 0, 0, 0, 1, 0, 4, 2, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 8, 0, 0, 0, 0, 4, 0, 0, 1, 0, 0, 0, 4, 1, 0, 0, 1,\n",
       "        3, 0, 0, 0, 8, 0, 4, 2, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 8, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 1, 0,\n",
       "        0, 0, 4, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1,\n",
       "        1, 0, 0, 2, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 8, 0, 0, 0, 1, 0,\n",
       "        2, 0, 0, 2, 1, 0, 1, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "        3, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 5, 0, 0, 0, 1, 0, 2, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 3, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 2, 2, 1, 0,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 5, 0, 0, 0,\n",
       "        1, 3, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 1, 1, 0, 1, 0, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "        1, 1, 2, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1,\n",
       "        0, 1, 0, 0, 3, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0,\n",
       "        2, 0, 0, 0, 1, 2, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 5, 1, 1, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 1, 0, 1, 1, 0,\n",
       "        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 4, 1, 0, 0, 0,\n",
       "        8, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 4,\n",
       "        0, 0, 0, 1, 0, 3, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 0, 0, 1, 4, 0, 1, 0, 1, 0, 1, 0,\n",
       "        0, 0, 2, 1, 0, 8, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       " 'Parch': array([0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 1,\n",
       "        0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0,\n",
       "        2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, 2, 0, 0, 0, 0, 2,\n",
       "        0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 2, 1, 4, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "        1, 0, 0, 0, 2, 0, 2, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 2, 2, 0, 0, 0, 1, 0, 2, 1, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "        0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2,\n",
       "        0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3, 4, 0,\n",
       "        1, 0, 0, 0, 0, 2, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "        2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 1, 1, 0, 1, 2, 0, 2, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1,\n",
       "        2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 2,\n",
       "        0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, 0, 0, 2,\n",
       "        1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1,\n",
       "        5, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 6, 1, 0, 0,\n",
       "        0, 2, 1, 2, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0,\n",
       "        0, 0, 1, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 2, 0, 0, 0,\n",
       "        2, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "        0, 0, 0, 1, 0, 2, 1, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 1, 1, 0, 1, 1, 0,\n",
       "        3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 5, 0, 0, 2, 0, 0]),\n",
       " 'Ticket': array(['A/5 21171', 'PC 17599', 'STON/O2. 3101282', '113803', '373450',\n",
       "        '330877', '17463', '349909', '347742', '237736', 'PP 9549',\n",
       "        '113783', 'A/5. 2151', '347082', '350406', '248706', '382652',\n",
       "        '244373', '345763', '2649', '239865', '248698', '330923', '113788',\n",
       "        '349909', '347077', '2631', '19950', '330959', '349216',\n",
       "        'PC 17601', 'PC 17569', '335677', 'C.A. 24579', 'PC 17604',\n",
       "        '113789', '2677', 'A./5. 2152', '345764', '2651', '7546', '11668',\n",
       "        '349253', 'SC/Paris 2123', '330958', 'S.C./A.4. 23567', '370371',\n",
       "        '14311', '2662', '349237', '3101295', 'A/4. 39886', 'PC 17572',\n",
       "        '2926', '113509', '19947', 'C.A. 31026', '2697', 'C.A. 34651',\n",
       "        'CA 2144', '2669', '113572', '36973', '347088', 'PC 17605', '2661',\n",
       "        'C.A. 29395', 'S.P. 3464', '3101281', '315151', 'C.A. 33111',\n",
       "        'CA 2144', 'S.O.C. 14879', '2680', '1601', '348123', '349208',\n",
       "        '374746', '248738', '364516', '345767', '345779', '330932',\n",
       "        '113059', 'SO/C 14885', '3101278', 'W./C. 6608', 'SOTON/OQ 392086',\n",
       "        '19950', '343275', '343276', '347466', 'W.E.P. 5734', 'C.A. 2315',\n",
       "        '364500', '374910', 'PC 17754', 'PC 17759', '231919', '244367',\n",
       "        '349245', '349215', '35281', '7540', '3101276', '349207', '343120',\n",
       "        '312991', '349249', '371110', '110465', '2665', '324669', '4136',\n",
       "        '2627', 'STON/O 2. 3101294', '370369', '11668', 'PC 17558',\n",
       "        '347082', 'S.O.C. 14879', 'A4. 54510', '237736', '27267', '35281',\n",
       "        '2651', '370372', 'C 17369', '2668', '347061', '349241',\n",
       "        'SOTON/O.Q. 3101307', 'A/5. 3337', '228414', 'C.A. 29178',\n",
       "        'SC/PARIS 2133', '11752', '113803', '7534', 'PC 17593', '2678',\n",
       "        '347081', 'STON/O2. 3101279', '365222', '231945', 'C.A. 33112',\n",
       "        '350043', 'W./C. 6608', '230080', '244310', 'S.O.P. 1166',\n",
       "        '113776', 'A.5. 11206', 'A/5. 851', 'Fa 265302', 'PC 17597',\n",
       "        '35851', 'SOTON/OQ 392090', '315037', 'CA. 2343', '371362',\n",
       "        'C.A. 33595', '347068', '315093', '3101295', '363291', '113505',\n",
       "        '347088', 'PC 17318', '1601', '111240', '382652', '347742',\n",
       "        'STON/O 2. 3101280', '17764', '350404', '4133', 'PC 17595',\n",
       "        '250653', 'LINE', 'CA. 2343', 'SC/PARIS 2131', '347077', '230136',\n",
       "        '315153', '113767', '370365', '111428', '364849', '349247',\n",
       "        '234604', '28424', '350046', '230080', 'PC 17610', 'PC 17569',\n",
       "        '368703', '4579', '370370', '248747', '345770', 'CA. 2343',\n",
       "        '3101264', '2628', 'A/5 3540', '347054', '3101278', '2699',\n",
       "        '367231', '112277', 'SOTON/O.Q. 3101311', 'F.C.C. 13528',\n",
       "        'A/5 21174', '250646', '367229', '35273', 'STON/O2. 3101283',\n",
       "        '243847', '11813', 'W/C 14208', 'SOTON/OQ 392089', '220367',\n",
       "        '21440', '349234', '19943', 'PP 4348', 'SW/PP 751', 'A/5 21173',\n",
       "        '236171', '4133', '36973', '347067', '237442', '347077',\n",
       "        'C.A. 29566', 'W./C. 6609', '26707', 'C.A. 31921', '28665',\n",
       "        'SCO/W 1585', '2665', '367230', 'W./C. 14263', 'STON/O 2. 3101275',\n",
       "        '2694', '19928', '347071', '250649', '11751', '244252', '362316',\n",
       "        '347054', '113514', 'A/5. 3336', '370129', '2650', 'PC 17585',\n",
       "        '110152', 'PC 17755', '230433', '384461', '347077', '110413',\n",
       "        '112059', '382649', 'C.A. 17248', '3101295', '347083', 'PC 17582',\n",
       "        'PC 17760', '113798', 'LINE', '250644', 'PC 17596', '370375',\n",
       "        '13502', '347073', '239853', '382652', 'C.A. 2673', '336439',\n",
       "        '347464', '345778', 'A/5. 10482', '113056', '349239', '345774',\n",
       "        '349206', '237798', '370373', '19877', '11967', 'SC/Paris 2163',\n",
       "        '349236', '349233', 'PC 17612', '2693', '113781', '19988',\n",
       "        'PC 17558', '9234', '367226', 'LINE', '226593', 'A/5 2466',\n",
       "        '113781', '17421', 'PC 17758', 'P/PP 3381', 'PC 17485', '11767',\n",
       "        'PC 17608', '250651', '349243', 'F.C.C. 13529', '347470', '244367',\n",
       "        '29011', '36928', '16966', 'A/5 21172', '349219', '234818',\n",
       "        '248738', 'CA. 2343', 'PC 17760', '345364', '28551', '363291',\n",
       "        '111361', '367226', '113043', 'PC 17582', '345764', 'PC 17611',\n",
       "        '349225', '113776', '16966', '7598', '113784', '230080', '19950',\n",
       "        '248740', '244361', '229236', '248733', '31418', '386525',\n",
       "        'C.A. 37671', '315088', '7267', '113510', '2695', '349237', '2647',\n",
       "        '345783', '113505', '237671', '330931', '330980', '347088',\n",
       "        'SC/PARIS 2167', '2691', 'SOTON/O.Q. 3101310', '370365', 'C 7076',\n",
       "        '110813', '2626', '14313', 'PC 17477', '11765', '3101267',\n",
       "        '323951', 'PC 17760', '349909', 'PC 17604', 'C 7077', '113503',\n",
       "        '2648', '347069', 'PC 17757', '2653', 'STON/O 2. 3101293',\n",
       "        '113789', '349227', 'S.O.C. 14879', 'CA 2144', '27849', '367655',\n",
       "        'SC 1748', '113760', '350034', '3101277', '35273', 'PP 9549',\n",
       "        '350052', '350407', '28403', '244278', '240929',\n",
       "        'STON/O 2. 3101289', '341826', '4137', 'STON/O2. 3101279',\n",
       "        '315096', '28664', '347064', '29106', '312992', '4133', '349222',\n",
       "        '394140', '19928', '239853', 'STON/O 2. 3101269', '343095',\n",
       "        '28220', '250652', '28228', '345773', '349254', 'A/5. 13032',\n",
       "        '315082', '347080', '370129', 'A/4. 34244', '2003', '250655',\n",
       "        '364851', 'SOTON/O.Q. 392078', '110564', '376564', 'SC/AH 3085',\n",
       "        'STON/O 2. 3101274', '13507', '113760', 'W./C. 6608', '29106',\n",
       "        '19950', 'C.A. 18723', 'F.C.C. 13529', '345769', '347076',\n",
       "        '230434', '65306', '33638', '250644', '113794', '2666', '113786',\n",
       "        'C.A. 34651', '65303', '113051', '17453', 'A/5 2817', '349240',\n",
       "        '13509', '17464', 'F.C.C. 13531', '371060', '19952', '364506',\n",
       "        '111320', '234360', 'A/S 2816', 'SOTON/O.Q. 3101306', '239853',\n",
       "        '113792', '36209', '2666', '323592', '315089', 'C.A. 34651',\n",
       "        'SC/AH Basle 541', '7553', '110465', '31027', '3460', '350060',\n",
       "        '3101298', 'CA 2144', '239854', 'A/5 3594', '4134', '11967',\n",
       "        '4133', '19943', '11771', 'A.5. 18509', 'C.A. 37671', '65304',\n",
       "        'SOTON/OQ 3101317', '113787', 'PC 17609', 'A/4 45380', '2627',\n",
       "        '36947', 'C.A. 6212', '113781', '350035', '315086', '364846',\n",
       "        '330909', '4135', '110152', 'PC 17758', '26360', '111427',\n",
       "        'C 4001', '1601', '382651', 'SOTON/OQ 3101316', 'PC 17473',\n",
       "        'PC 17603', '349209', '36967', 'C.A. 34260', '371110', '226875',\n",
       "        '349242', '12749', '349252', '2624', '111361', '2700', '367232',\n",
       "        'W./C. 14258', 'PC 17483', '3101296', '29104', '26360', '2641',\n",
       "        '2690', '2668', '315084', 'F.C.C. 13529', '113050', 'PC 17761',\n",
       "        '364498', '13568', 'WE/P 5735', '347082', '347082', '2908',\n",
       "        'PC 17761', '693', '2908', 'SC/PARIS 2146', '363291', 'C.A. 33112',\n",
       "        '17421', '244358', '330979', '2620', '347085', '113807', '11755',\n",
       "        'PC 17757', '110413', '345572', '372622', '349251', '218629',\n",
       "        'SOTON/OQ 392082', 'SOTON/O.Q. 392087', 'A/4 48871', '349205',\n",
       "        '349909', '2686', '350417', 'S.W./PP 752', '11769', 'PC 17474',\n",
       "        '14312', 'A/4. 20589', '358585', '243880', '13507', '2689',\n",
       "        'STON/O 2. 3101286', '237789', '17421', '28403', '13049', '3411',\n",
       "        '110413', '237565', '13567', '14973', 'A./5. 3235',\n",
       "        'STON/O 2. 3101273', '36947', 'A/5 3902', '364848', 'SC/AH 29037',\n",
       "        '345773', '248727', 'LINE', '2664', 'PC 17485', '243847', '349214',\n",
       "        '113796', '364511', '111426', '349910', '349246', '113804',\n",
       "        'SC/Paris 2123', 'PC 17582', '347082', 'SOTON/O.Q. 3101305',\n",
       "        '367230', '370377', '364512', '220845', '347080', 'A/5. 3336',\n",
       "        '230136', '31028', '2659', '11753', '2653', '350029', '54636',\n",
       "        '36963', '219533', '13502', '349224', '334912', '27042', '347743',\n",
       "        '13214', '112052', '347088', '237668', 'STON/O 2. 3101292',\n",
       "        'C.A. 31921', '3101295', '376564', '350050', 'PC 17477', '347088',\n",
       "        '1601', '2666', 'PC 17572', '349231', '13213', 'S.O./P.P. 751',\n",
       "        'CA. 2314', '349221', '231919', '8475', '330919', '365226',\n",
       "        'S.O.C. 14879', '349223', '364849', '29751', '35273', 'PC 17611',\n",
       "        '2623', '5727', '349210', 'STON/O 2. 3101285', 'S.O.C. 14879',\n",
       "        '234686', '312993', 'A/5 3536', '19996', '29750', 'F.C. 12750',\n",
       "        'C.A. 24580', '244270', '239856', '349912', '342826', '4138',\n",
       "        'CA 2144', 'PC 17755', '330935', 'PC 17572', '6563', 'CA 2144',\n",
       "        '29750', 'SC/Paris 2123', '3101295', '349228', '350036', '24160',\n",
       "        '17474', '349256', '1601', '2672', '113800', '248731', '363592',\n",
       "        '35852', '17421', '348121', 'PC 17757', 'PC 17475', '2691',\n",
       "        '36864', '350025', '250655', '223596', 'PC 17476', '113781',\n",
       "        '2661', 'PC 17482', '113028', '19996', '7545', '250647', '348124',\n",
       "        'PC 17757', '34218', '36568', '347062', '248727', '350048',\n",
       "        '12233', '250643', '113806', '315094', '31027', '36866', '236853',\n",
       "        'STON/O2. 3101271', '24160', '2699', '239855', '28425', '233639',\n",
       "        '54636', 'W./C. 6608', 'PC 17755', '349201', '349218', '16988',\n",
       "        '19877', 'PC 17608', '376566', 'STON/O 2. 3101288', 'WE/P 5735',\n",
       "        'C.A. 2673', '250648', '113773', '335097', '29103', '392096',\n",
       "        '345780', '349204', '220845', '250649', '350042', '29108',\n",
       "        '363294', '110152', '358585', 'SOTON/O2 3101272', '2663', '113760',\n",
       "        '347074', '13502', '112379', '364850', '371110', '8471', '345781',\n",
       "        '350047', 'S.O./P.P. 3', '2674', '29105', '347078', '383121',\n",
       "        '364516', '36865', '24160', '2687', '17474', '113501',\n",
       "        'W./C. 6607', 'SOTON/O.Q. 3101312', '374887', '3101265', '382652',\n",
       "        'C.A. 2315', 'PC 17593', '12460', '239865', 'CA. 2343', 'PC 17600',\n",
       "        '349203', '28213', '17465', '349244', '2685', '345773', '250647',\n",
       "        'C.A. 31921', '113760', '2625', '347089', '347063', '112050',\n",
       "        '347087', '248723', '113806', '3474', 'A/4 48871', '28206',\n",
       "        '347082', '364499', '112058', 'STON/O2. 3101290',\n",
       "        'S.C./PARIS 2079', 'C 7075', '347088', '12749', '315098', '19972',\n",
       "        '392096', '3101295', '368323', '1601', 'S.C./PARIS 2079', '367228',\n",
       "        '113572', '2659', '29106', '2671', '347468', '2223', 'PC 17756',\n",
       "        '315097', '392092', '1601', '11774', 'SOTON/O2 3101287',\n",
       "        'S.O./P.P. 3', '113798', '2683', '315090', 'C.A. 5547', 'CA. 2343',\n",
       "        '349213', '248727', '17453', '347082', '347060', '2678',\n",
       "        'PC 17592', '244252', '392091', '36928', '113055', '2666', '2629',\n",
       "        '350026', '28134', '17466', 'CA. 2343', '233866', '236852',\n",
       "        'SC/PARIS 2149', 'PC 17590', '345777', '347742', '349248', '11751',\n",
       "        '695', '345765', 'P/PP 3381', '2667', '7534', '349212', '349217',\n",
       "        '11767', '230433', '349257', '7552', 'C.A./SOTON 34068',\n",
       "        'SOTON/OQ 392076', '382652', '211536', '112053', 'W./C. 6607',\n",
       "        '111369', '370376'], dtype='<U18'),\n",
       " 'Fare': array([  7.25  ,  71.2833,   7.925 ,  53.1   ,   8.05  ,   8.4583,\n",
       "         51.8625,  21.075 ,  11.1333,  30.0708,  16.7   ,  26.55  ,\n",
       "          8.05  ,  31.275 ,   7.8542,  16.    ,  29.125 ,  13.    ,\n",
       "         18.    ,   7.225 ,  26.    ,  13.    ,   8.0292,  35.5   ,\n",
       "         21.075 ,  31.3875,   7.225 , 263.    ,   7.8792,   7.8958,\n",
       "         27.7208, 146.5208,   7.75  ,  10.5   ,  82.1708,  52.    ,\n",
       "          7.2292,   8.05  ,  18.    ,  11.2417,   9.475 ,  21.    ,\n",
       "          7.8958,  41.5792,   7.8792,   8.05  ,  15.5   ,   7.75  ,\n",
       "         21.6792,  17.8   ,  39.6875,   7.8   ,  76.7292,  26.    ,\n",
       "         61.9792,  35.5   ,  10.5   ,   7.2292,  27.75  ,  46.9   ,\n",
       "          7.2292,  80.    ,  83.475 ,  27.9   ,  27.7208,  15.2458,\n",
       "         10.5   ,   8.1583,   7.925 ,   8.6625,  10.5   ,  46.9   ,\n",
       "         73.5   ,  14.4542,  56.4958,   7.65  ,   7.8958,   8.05  ,\n",
       "         29.    ,  12.475 ,   9.    ,   9.5   ,   7.7875,  47.1   ,\n",
       "         10.5   ,  15.85  ,  34.375 ,   8.05  , 263.    ,   8.05  ,\n",
       "          8.05  ,   7.8542,  61.175 ,  20.575 ,   7.25  ,   8.05  ,\n",
       "         34.6542,  63.3583,  23.    ,  26.    ,   7.8958,   7.8958,\n",
       "         77.2875,   8.6542,   7.925 ,   7.8958,   7.65  ,   7.775 ,\n",
       "          7.8958,  24.15  ,  52.    ,  14.4542,   8.05  ,   9.825 ,\n",
       "         14.4583,   7.925 ,   7.75  ,  21.    , 247.5208,  31.275 ,\n",
       "         73.5   ,   8.05  ,  30.0708,  13.    ,  77.2875,  11.2417,\n",
       "          7.75  ,   7.1417,  22.3583,   6.975 ,   7.8958,   7.05  ,\n",
       "         14.5   ,  26.    ,  13.    ,  15.0458,  26.2833,  53.1   ,\n",
       "          9.2167,  79.2   ,  15.2458,   7.75  ,  15.85  ,   6.75  ,\n",
       "         11.5   ,  36.75  ,   7.7958,  34.375 ,  26.    ,  13.    ,\n",
       "         12.525 ,  66.6   ,   8.05  ,  14.5   ,   7.3125,  61.3792,\n",
       "          7.7333,   8.05  ,   8.6625,  69.55  ,  16.1   ,  15.75  ,\n",
       "          7.775 ,   8.6625,  39.6875,  20.525 ,  55.    ,  27.9   ,\n",
       "         25.925 ,  56.4958,  33.5   ,  29.125 ,  11.1333,   7.925 ,\n",
       "         30.6958,   7.8542,  25.4667,  28.7125,  13.    ,   0.    ,\n",
       "         69.55  ,  15.05  ,  31.3875,  39.    ,  22.025 ,  50.    ,\n",
       "         15.5   ,  26.55  ,  15.5   ,   7.8958,  13.    ,  13.    ,\n",
       "          7.8542,  26.    ,  27.7208, 146.5208,   7.75  ,   8.4042,\n",
       "          7.75  ,  13.    ,   9.5   ,  69.55  ,   6.4958,   7.225 ,\n",
       "          8.05  ,  10.4625,  15.85  ,  18.7875,   7.75  ,  31.    ,\n",
       "          7.05  ,  21.    ,   7.25  ,  13.    ,   7.75  , 113.275 ,\n",
       "          7.925 ,  27.    ,  76.2917,  10.5   ,   8.05  ,  13.    ,\n",
       "          8.05  ,   7.8958,  90.    ,   9.35  ,  10.5   ,   7.25  ,\n",
       "         13.    ,  25.4667,  83.475 ,   7.775 ,  13.5   ,  31.3875,\n",
       "         10.5   ,   7.55  ,  26.    ,  26.25  ,  10.5   ,  12.275 ,\n",
       "         14.4542,  15.5   ,  10.5   ,   7.125 ,   7.225 ,  90.    ,\n",
       "          7.775 ,  14.5   ,  52.5542,  26.    ,   7.25  ,  10.4625,\n",
       "         26.55  ,  16.1   ,  20.2125,  15.2458,  79.2   ,  86.5   ,\n",
       "        512.3292,  26.    ,   7.75  ,  31.3875,  79.65  ,   0.    ,\n",
       "          7.75  ,  10.5   ,  39.6875,   7.775 , 153.4625, 135.6333,\n",
       "         31.    ,   0.    ,  19.5   ,  29.7   ,   7.75  ,  77.9583,\n",
       "          7.75  ,   0.    ,  29.125 ,  20.25  ,   7.75  ,   7.8542,\n",
       "          9.5   ,   8.05  ,  26.    ,   8.6625,   9.5   ,   7.8958,\n",
       "         13.    ,   7.75  ,  78.85  ,  91.0792,  12.875 ,   8.85  ,\n",
       "          7.8958,  27.7208,   7.2292, 151.55  ,  30.5   , 247.5208,\n",
       "          7.75  ,  23.25  ,   0.    ,  12.35  ,   8.05  , 151.55  ,\n",
       "        110.8833, 108.9   ,  24.    ,  56.9292,  83.1583, 262.375 ,\n",
       "         26.    ,   7.8958,  26.25  ,   7.8542,  26.    ,  14.    ,\n",
       "        164.8667, 134.5   ,   7.25  ,   7.8958,  12.35  ,  29.    ,\n",
       "         69.55  , 135.6333,   6.2375,  13.    ,  20.525 ,  57.9792,\n",
       "         23.25  ,  28.5   , 153.4625,  18.    , 133.65  ,   7.8958,\n",
       "         66.6   , 134.5   ,   8.05  ,  35.5   ,  26.    , 263.    ,\n",
       "         13.    ,  13.    ,  13.    ,  13.    ,  13.    ,  16.1   ,\n",
       "         15.9   ,   8.6625,   9.225 ,  35.    ,   7.2292,  17.8   ,\n",
       "          7.225 ,   9.5   ,  55.    ,  13.    ,   7.8792,   7.8792,\n",
       "         27.9   ,  27.7208,  14.4542,   7.05  ,  15.5   ,   7.25  ,\n",
       "         75.25  ,   7.2292,   7.75  ,  69.3   ,  55.4417,   6.4958,\n",
       "          8.05  , 135.6333,  21.075 ,  82.1708,   7.25  , 211.5   ,\n",
       "          4.0125,   7.775 , 227.525 ,  15.7417,   7.925 ,  52.    ,\n",
       "          7.8958,  73.5   ,  46.9   ,  13.    ,   7.7292,  12.    ,\n",
       "        120.    ,   7.7958,   7.925 , 113.275 ,  16.7   ,   7.7958,\n",
       "          7.8542,  26.    ,  10.5   ,  12.65  ,   7.925 ,   8.05  ,\n",
       "          9.825 ,  15.85  ,   8.6625,  21.    ,   7.75  ,  18.75  ,\n",
       "          7.775 ,  25.4667,   7.8958,   6.8583,  90.    ,   0.    ,\n",
       "          7.925 ,   8.05  ,  32.5   ,  13.    ,  13.    ,  24.15  ,\n",
       "          7.8958,   7.7333,   7.875 ,  14.4   ,  20.2125,   7.25  ,\n",
       "         26.    ,  26.    ,   7.75  ,   8.05  ,  26.55  ,  16.1   ,\n",
       "         26.    ,   7.125 ,  55.9   , 120.    ,  34.375 ,  18.75  ,\n",
       "        263.    ,  10.5   ,  26.25  ,   9.5   ,   7.775 ,  13.    ,\n",
       "          8.1125,  81.8583,  19.5   ,  26.55  ,  19.2583,  30.5   ,\n",
       "         27.75  ,  19.9667,  27.75  ,  89.1042,   8.05  ,   7.8958,\n",
       "         26.55  ,  51.8625,  10.5   ,   7.75  ,  26.55  ,   8.05  ,\n",
       "         38.5   ,  13.    ,   8.05  ,   7.05  ,   0.    ,  26.55  ,\n",
       "          7.725 ,  19.2583,   7.25  ,   8.6625,  27.75  ,  13.7917,\n",
       "          9.8375,  52.    ,  21.    ,   7.0458,   7.5208,  12.2875,\n",
       "         46.9   ,   0.    ,   8.05  ,   9.5875,  91.0792,  25.4667,\n",
       "         90.    ,  29.7   ,   8.05  ,  15.9   ,  19.9667,   7.25  ,\n",
       "         30.5   ,  49.5042,   8.05  ,  14.4583,  78.2667,  15.1   ,\n",
       "        151.55  ,   7.7958,   8.6625,   7.75  ,   7.6292,   9.5875,\n",
       "         86.5   , 108.9   ,  26.    ,  26.55  ,  22.525 ,  56.4958,\n",
       "          7.75  ,   8.05  ,  26.2875,  59.4   ,   7.4958,  34.0208,\n",
       "         10.5   ,  24.15  ,  26.    ,   7.8958,  93.5   ,   7.8958,\n",
       "          7.225 ,  57.9792,   7.2292,   7.75  ,  10.5   , 221.7792,\n",
       "          7.925 ,  11.5   ,  26.    ,   7.2292,   7.2292,  22.3583,\n",
       "          8.6625,  26.25  ,  26.55  , 106.425 ,  14.5   ,  49.5   ,\n",
       "         71.    ,  31.275 ,  31.275 ,  26.    , 106.425 ,  26.    ,\n",
       "         26.    ,  13.8625,  20.525 ,  36.75  , 110.8833,  26.    ,\n",
       "          7.8292,   7.225 ,   7.775 ,  26.55  ,  39.6   , 227.525 ,\n",
       "         79.65  ,  17.4   ,   7.75  ,   7.8958,  13.5   ,   8.05  ,\n",
       "          8.05  ,  24.15  ,   7.8958,  21.075 ,   7.2292,   7.8542,\n",
       "         10.5   ,  51.4792,  26.3875,   7.75  ,   8.05  ,  14.5   ,\n",
       "         13.    ,  55.9   ,  14.4583,   7.925 ,  30.    , 110.8833,\n",
       "         26.    ,  40.125 ,   8.7125,  79.65  ,  15.    ,  79.2   ,\n",
       "          8.05  ,   8.05  ,   7.125 ,  78.2667,   7.25  ,   7.75  ,\n",
       "         26.    ,  24.15  ,  33.    ,   0.    ,   7.225 ,  56.9292,\n",
       "         27.    ,   7.8958,  42.4   ,   8.05  ,  26.55  ,  15.55  ,\n",
       "          7.8958,  30.5   ,  41.5792, 153.4625,  31.275 ,   7.05  ,\n",
       "         15.5   ,   7.75  ,   8.05  ,  65.    ,  14.4   ,  16.1   ,\n",
       "         39.    ,  10.5   ,  14.4542,  52.5542,  15.7417,   7.8542,\n",
       "         16.1   ,  32.3208,  12.35  ,  77.9583,   7.8958,   7.7333,\n",
       "         30.    ,   7.0542,  30.5   ,   0.    ,  27.9   ,  13.    ,\n",
       "          7.925 ,  26.25  ,  39.6875,  16.1   ,   7.8542,  69.3   ,\n",
       "         27.9   ,  56.4958,  19.2583,  76.7292,   7.8958,  35.5   ,\n",
       "          7.55  ,   7.55  ,   7.8958,  23.    ,   8.4333,   7.8292,\n",
       "          6.75  ,  73.5   ,   7.8958,  15.5   ,  13.    , 113.275 ,\n",
       "        133.65  ,   7.225 ,  25.5875,   7.4958,   7.925 ,  73.5   ,\n",
       "         13.    ,   7.775 ,   8.05  ,  52.    ,  39.    ,  52.    ,\n",
       "         10.5   ,  13.    ,   0.    ,   7.775 ,   8.05  ,   9.8417,\n",
       "         46.9   , 512.3292,   8.1375,  76.7292,   9.225 ,  46.9   ,\n",
       "         39.    ,  41.5792,  39.6875,  10.1708,   7.7958, 211.3375,\n",
       "         57.    ,  13.4167,  56.4958,   7.225 ,  26.55  ,  13.5   ,\n",
       "          8.05  ,   7.7333, 110.8833,   7.65  , 227.525 ,  26.2875,\n",
       "         14.4542,   7.7417,   7.8542,  26.    ,  13.5   ,  26.2875,\n",
       "        151.55  ,  15.2458,  49.5042,  26.55  ,  52.    ,   9.4833,\n",
       "         13.    ,   7.65  , 227.525 ,  10.5   ,  15.5   ,   7.775 ,\n",
       "         33.    ,   7.0542,  13.    ,  13.    ,  53.1   ,   8.6625,\n",
       "         21.    ,   7.7375,  26.    ,   7.925 , 211.3375,  18.7875,\n",
       "          0.    ,  13.    ,  13.    ,  16.1   ,  34.375 , 512.3292,\n",
       "          7.8958,   7.8958,  30.    ,  78.85  , 262.375 ,  16.1   ,\n",
       "          7.925 ,  71.    ,  20.25  ,  13.    ,  53.1   ,   7.75  ,\n",
       "         23.    ,  12.475 ,   9.5   ,   7.8958,  65.    ,  14.5   ,\n",
       "          7.7958,  11.5   ,   8.05  ,  86.5   ,  14.5   ,   7.125 ,\n",
       "          7.2292, 120.    ,   7.775 ,  77.9583,  39.6   ,   7.75  ,\n",
       "         24.15  ,   8.3625,   9.5   ,   7.8542,  10.5   ,   7.225 ,\n",
       "         23.    ,   7.75  ,   7.75  ,  12.475 ,   7.7375, 211.3375,\n",
       "          7.2292,  57.    ,  30.    ,  23.45  ,   7.05  ,   7.25  ,\n",
       "          7.4958,  29.125 ,  20.575 ,  79.2   ,   7.75  ,  26.    ,\n",
       "         69.55  ,  30.6958,   7.8958,  13.    ,  25.9292,   8.6833,\n",
       "          7.2292,  24.15  ,  13.    ,  26.25  , 120.    ,   8.5167,\n",
       "          6.975 ,   7.775 ,   0.    ,   7.775 ,  13.    ,  53.1   ,\n",
       "          7.8875,  24.15  ,  10.5   ,  31.275 ,   8.05  ,   0.    ,\n",
       "          7.925 ,  37.0042,   6.45  ,  27.9   ,  93.5   ,   8.6625,\n",
       "          0.    ,  12.475 ,  39.6875,   6.95  ,  56.4958,  37.0042,\n",
       "          7.75  ,  80.    ,  14.4542,  18.75  ,   7.2292,   7.8542,\n",
       "          8.3   ,  83.1583,   8.6625,   8.05  ,  56.4958,  29.7   ,\n",
       "          7.925 ,  10.5   ,  31.    ,   6.4375,   8.6625,   7.55  ,\n",
       "         69.55  ,   7.8958,  33.    ,  89.1042,  31.275 ,   7.775 ,\n",
       "         15.2458,  39.4   ,  26.    ,   9.35  , 164.8667,  26.55  ,\n",
       "         19.2583,   7.2292,  14.1083,  11.5   ,  25.9292,  69.55  ,\n",
       "         13.    ,  13.    ,  13.8583,  50.4958,   9.5   ,  11.1333,\n",
       "          7.8958,  52.5542,   5.    ,   9.    ,  24.    ,   7.225 ,\n",
       "          9.8458,   7.8958,   7.8958,  83.1583,  26.    ,   7.8958,\n",
       "         10.5167,  10.5   ,   7.05  ,  29.125 ,  13.    ,  30.    ,\n",
       "         23.45  ,  30.    ,   7.75  ]),\n",
       " 'Cabin': array(['nan', 'C85', 'nan', 'C123', 'nan', 'nan', 'E46', 'nan', 'nan',\n",
       "        'nan', 'G6', 'C103', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'D56', 'nan', 'A6', 'nan', 'nan', 'nan',\n",
       "        'C23 C25 C27', 'nan', 'nan', 'nan', 'B78', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'D33',\n",
       "        'nan', 'B30', 'C52', 'nan', 'nan', 'nan', 'nan', 'nan', 'B28',\n",
       "        'C83', 'nan', 'nan', 'nan', 'F33', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'F G73', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'C23 C25 C27', 'nan', 'nan', 'nan', 'E31', 'nan', 'nan', 'nan',\n",
       "        'A5', 'D10 D12', 'nan', 'nan', 'nan', 'nan', 'D26', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'C110', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'B58 B60', 'nan', 'nan', 'nan', 'nan',\n",
       "        'E101', 'D26', 'nan', 'nan', 'nan', 'F E69', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'D47', 'C123', 'nan', 'B86', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'F2', 'nan',\n",
       "        'nan', 'C2', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'E33', 'nan',\n",
       "        'nan', 'nan', 'B19', 'nan', 'nan', 'nan', 'A7', 'nan', 'nan',\n",
       "        'C49', 'nan', 'nan', 'nan', 'nan', 'nan', 'F4', 'nan', 'A32',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'F2', 'B4', 'B80',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'G6', 'nan', 'nan', 'nan', 'A31', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'D36', 'nan', 'nan', 'D15', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'C93', 'nan', 'nan', 'nan', 'nan', 'nan', 'C83', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'C78', 'nan', 'nan', 'D35', 'nan',\n",
       "        'nan', 'G6', 'C87', 'nan', 'nan', 'nan', 'nan', 'B77', 'nan',\n",
       "        'nan', 'nan', 'nan', 'E67', 'B94', 'nan', 'nan', 'nan', 'nan',\n",
       "        'C125', 'C99', 'nan', 'nan', 'nan', 'C118', 'nan', 'D7', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'A19', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'B49', 'D', 'nan', 'nan', 'nan',\n",
       "        'nan', 'C22 C26', 'C106', 'B58 B60', 'nan', 'nan', 'nan', 'E101',\n",
       "        'nan', 'C22 C26', 'nan', 'C65', 'nan', 'E36', 'C54',\n",
       "        'B57 B59 B63 B66', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'C7',\n",
       "        'E34', 'nan', 'nan', 'nan', 'nan', 'nan', 'C32', 'nan', 'D', 'nan',\n",
       "        'B18', 'nan', 'C124', 'C91', 'nan', 'nan', 'nan', 'C2', 'E40',\n",
       "        'nan', 'T', 'F2', 'C23 C25 C27', 'nan', 'nan', 'nan', 'F33', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'C128', 'nan', 'nan', 'nan', 'nan',\n",
       "        'E33', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'D37', 'nan', 'nan', 'B35', 'E50', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'C82', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'B96 B98', 'nan',\n",
       "        'nan', 'D36', 'G6', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'C78', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'E10', 'C52', 'nan', 'nan', 'nan', 'E44', 'B96 B98', 'nan',\n",
       "        'nan', 'C23 C25 C27', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'A34', 'nan', 'nan', 'nan', 'C104', 'nan', 'nan', 'C111', 'C92',\n",
       "        'nan', 'nan', 'E38', 'D21', 'nan', 'nan', 'E12', 'nan', 'E63',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'D', 'nan', 'A14', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'B49', 'nan', 'C93', 'B37', 'nan', 'nan', 'nan',\n",
       "        'nan', 'C30', 'nan', 'nan', 'nan', 'D20', 'nan', 'C22 C26', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'B79', 'C65', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'E25', 'nan', 'nan', 'D46', 'F33', 'nan',\n",
       "        'nan', 'nan', 'B73', 'nan', 'nan', 'B18', 'nan', 'nan', 'nan',\n",
       "        'C95', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'B38', 'nan', 'nan', 'B39', 'B22', 'nan', 'nan', 'nan', 'C86',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'C70', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'A16', 'nan', 'E67', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'C101',\n",
       "        'E25', 'nan', 'nan', 'nan', 'nan', 'E44', 'nan', 'nan', 'nan',\n",
       "        'C68', 'nan', 'A10', 'nan', 'E68', 'nan', 'B41', 'nan', 'nan',\n",
       "        'nan', 'D20', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'A20', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'C125', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'F4', 'nan', 'nan', 'D19', 'nan', 'nan', 'nan', 'D50',\n",
       "        'nan', 'D9', 'nan', 'nan', 'A23', 'nan', 'B50', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'B35', 'nan', 'nan',\n",
       "        'nan', 'D33', 'nan', 'A26', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'D48', 'nan', 'nan',\n",
       "        'E58', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'C126', 'nan',\n",
       "        'B71', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'B51 B53 B55', 'nan', 'D49', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'B5', 'B20', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'C68', 'F G63', 'C62 C64', 'E24', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'E24', 'nan', 'nan', 'C90', 'C124', 'C126',\n",
       "        'nan', 'nan', 'F G73', 'C45', 'E101', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'E8', 'nan', 'nan', 'nan', 'nan', 'nan', 'B5', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'B101', 'nan', 'nan', 'D45',\n",
       "        'C46', 'B57 B59 B63 B66', 'nan', 'nan', 'B22', 'nan', 'nan', 'D30',\n",
       "        'nan', 'nan', 'E121', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'B77', 'nan', 'nan', 'nan', 'B96 B98', 'nan', 'D11', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'E77', 'nan', 'nan', 'nan',\n",
       "        'F38', 'nan', 'nan', 'B3', 'nan', 'B20', 'D6', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'B82 B84', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'D17', 'nan', 'nan', 'nan', 'nan', 'nan', 'B96 B98', 'nan',\n",
       "        'nan', 'nan', 'A36', 'nan', 'nan', 'E8', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'B102', 'nan', 'nan', 'nan', 'nan', 'B69', 'nan',\n",
       "        'nan', 'E121', 'nan', 'nan', 'nan', 'nan', 'nan', 'B28', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'E49', 'nan', 'nan', 'nan', 'C47',\n",
       "        'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan', 'nan',\n",
       "        'C92', 'nan', 'nan', 'nan', 'D28', 'nan', 'nan', 'nan', 'E17',\n",
       "        'nan', 'nan', 'nan', 'nan', 'D17', 'nan', 'nan', 'nan', 'nan',\n",
       "        'A24', 'nan', 'nan', 'nan', 'D35', 'B51 B53 B55', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'nan', 'C50', 'nan', 'nan', 'nan', 'nan',\n",
       "        'nan', 'nan', 'nan', 'B42', 'nan', 'C148', 'nan'], dtype='<U32'),\n",
       " 'Embarked': array(['S', 'C', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'C', 'S', 'S', 'S',\n",
       "        'S', 'S', 'S', 'Q', 'S', 'S', 'C', 'S', 'S', 'Q', 'S', 'S', 'S',\n",
       "        'C', 'S', 'Q', 'S', 'C', 'C', 'Q', 'S', 'C', 'S', 'C', 'S', 'S',\n",
       "        'C', 'S', 'S', 'C', 'C', 'Q', 'S', 'Q', 'Q', 'C', 'S', 'S', 'S',\n",
       "        'C', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'C', 'nan', 'S', 'S', 'C',\n",
       "        'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S',\n",
       "        'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "        'S', 'S', 'S', 'S', 'S', 'C', 'C', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "        'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'C', 'S', 'S', 'C', 'S', 'Q',\n",
       "        'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'Q', 'S', 'C', 'S',\n",
       "        'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'C', 'S', 'S',\n",
       "        'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C',\n",
       "        'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "        'S', 'S', 'Q', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'C',\n",
       "        'S', 'S', 'S', 'S', 'Q', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'C',\n",
       "        'C', 'Q', 'S', 'Q', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'C',\n",
       "        'Q', 'C', 'S', 'S', 'S', 'S', 'Q', 'C', 'S', 'S', 'C', 'S', 'S',\n",
       "        'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "        'S', 'S', 'S', 'S', 'S', 'S', 'C', 'Q', 'S', 'S', 'C', 'Q', 'S',\n",
       "        'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'S', 'C', 'S',\n",
       "        'Q', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "        'C', 'Q', 'S', 'S', 'S', 'Q', 'S', 'Q', 'S', 'S', 'S', 'S', 'C',\n",
       "        'S', 'S', 'S', 'Q', 'S', 'C', 'C', 'S', 'S', 'C', 'C', 'S', 'S',\n",
       "        'C', 'Q', 'Q', 'S', 'Q', 'S', 'S', 'C', 'C', 'C', 'C', 'C', 'C',\n",
       "        'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'Q', 'S', 'S',\n",
       "        'C', 'S', 'S', 'S', 'C', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'C',\n",
       "        'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "        'S', 'C', 'S', 'C', 'S', 'S', 'S', 'Q', 'Q', 'S', 'C', 'C', 'S',\n",
       "        'Q', 'S', 'C', 'C', 'Q', 'C', 'C', 'S', 'S', 'C', 'S', 'C', 'S',\n",
       "        'C', 'C', 'S', 'C', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'C',\n",
       "        'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "        'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'Q', 'S', 'S', 'S',\n",
       "        'S', 'S', 'S', 'S', 'C', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'Q',\n",
       "        'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "        'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'C', 'S',\n",
       "        'C', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "        'Q', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "        'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'C',\n",
       "        'S', 'C', 'C', 'S', 'S', 'S', 'S', 'Q', 'Q', 'S', 'S', 'C', 'S',\n",
       "        'S', 'S', 'S', 'Q', 'S', 'S', 'C', 'S', 'S', 'S', 'Q', 'S', 'S',\n",
       "        'S', 'S', 'C', 'C', 'C', 'Q', 'S', 'S', 'S', 'S', 'S', 'C', 'C',\n",
       "        'C', 'S', 'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'S', 'C', 'S',\n",
       "        'S', 'C', 'S', 'S', 'C', 'S', 'Q', 'C', 'S', 'S', 'C', 'C', 'S',\n",
       "        'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S',\n",
       "        'S', 'Q', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'S', 'C', 'C',\n",
       "        'S', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'Q', 'S', 'S', 'S', 'S',\n",
       "        'C', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S',\n",
       "        'S', 'Q', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'C', 'S',\n",
       "        'S', 'S', 'Q', 'S', 'S', 'Q', 'S', 'S', 'C', 'S', 'S', 'S', 'S',\n",
       "        'S', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'C', 'S', 'C', 'S', 'S',\n",
       "        'S', 'S', 'S', 'Q', 'Q', 'S', 'S', 'Q', 'S', 'C', 'S', 'C', 'S',\n",
       "        'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S',\n",
       "        'S', 'S', 'S', 'C', 'Q', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'S',\n",
       "        'S', 'S', 'C', 'S', 'C', 'S', 'S', 'S', 'Q', 'C', 'S', 'C', 'S',\n",
       "        'C', 'Q', 'S', 'S', 'S', 'S', 'S', 'C', 'C', 'S', 'S', 'S', 'S',\n",
       "        'S', 'C', 'S', 'Q', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'Q',\n",
       "        'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S',\n",
       "        'S', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'S', 'S', 'S',\n",
       "        'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'C',\n",
       "        'Q', 'Q', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'Q', 'S', 'Q', 'S',\n",
       "        'C', 'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'C', 'Q', 'S', 'S',\n",
       "        'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'C', 'S', 'S',\n",
       "        'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S',\n",
       "        'S', 'S', 'S', 'S', 'S', 'S', 'Q', 'S', 'C', 'Q', 'nan', 'C', 'S',\n",
       "        'C', 'S', 'S', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'C', 'C', 'S',\n",
       "        'S', 'S', 'C', 'S', 'C', 'S', 'S', 'C', 'S', 'S', 'S', 'S', 'S',\n",
       "        'C', 'C', 'S', 'S', 'S', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S',\n",
       "        'S', 'S', 'S', 'C', 'C', 'S', 'S', 'S', 'C', 'S', 'S', 'S', 'S',\n",
       "        'S', 'Q', 'S', 'S', 'S', 'C', 'Q'], dtype='<U3')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = {}\n",
    "index = 0\n",
    "for cols in train.columns:\n",
    "    #print(cols)\n",
    "    if (not(index)) and (str(cols)!=\"PassengerId\" and str(cols)!=\"Name\"):\n",
    "        print(cols)\n",
    "        print(\"Blahblahblah\")\n",
    "        x[str(cols)] = np.array([i for i in train.loc[:,cols]])\n",
    "        index +=1\n",
    "    else:\n",
    "        if index and (str(cols)!=\"PassengerId\" and str(cols)!=\"Name\"):\n",
    "            print(cols)\n",
    "            x[str(cols)] =np.array([i for i in train.loc[:,cols]])\n",
    "            index +=1   \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, array(['C', 'Q', 'S', 'nan'], dtype='<U3'))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(x[\"Embarked\"],return_counts=True)\n",
    "len(dict(zip(unique, counts))), unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = dict(zip(unique, range(len(unique)))) if np.nan\n",
    "x[\"\"] = np.array([dict_[str(i)] for i in x[\"\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 3, 3, 3, 2, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 1, 3, 3,\n",
       "       2, 3, 3, 3, 1, 3, 2, 3, 1, 1, 2, 3, 1, 3, 1, 3, 3, 1, 3, 3, 1, 1,\n",
       "       2, 3, 2, 2, 1, 3, 3, 3, 1, 3, 1, 3, 3, 1, 3, 3, 1, 3, 3, 3, 1, 1,\n",
       "       3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2,\n",
       "       3, 1, 3, 3, 1, 3, 2, 3, 1, 3, 3, 3, 1, 3, 3, 1, 2, 3, 1, 3, 1, 3,\n",
       "       3, 3, 3, 1, 3, 3, 3, 1, 1, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 1, 3,\n",
       "       3, 1, 3, 3, 3, 1, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 3, 1, 1, 2, 3,\n",
       "       2, 3, 3, 3, 3, 1, 3, 3, 3, 1, 2, 1, 3, 3, 3, 3, 2, 1, 3, 3, 1, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2,\n",
       "       3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 1, 3, 2, 3, 3, 3,\n",
       "       2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 2, 3, 2, 3, 3, 3, 3, 1,\n",
       "       3, 3, 3, 2, 3, 1, 1, 3, 3, 1, 1, 3, 3, 1, 2, 2, 3, 2, 3, 3, 1, 1,\n",
       "       1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 2, 3, 3, 1, 3, 3, 3, 1,\n",
       "       2, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       1, 3, 1, 3, 3, 3, 2, 2, 3, 1, 1, 3, 2, 3, 1, 1, 2, 1, 1, 3, 3, 1,\n",
       "       3, 1, 3, 1, 1, 3, 1, 1, 3, 3, 3, 3, 3, 3, 2, 1, 3, 3, 3, 1, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3,\n",
       "       3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 1, 3, 1, 3, 3, 3, 2, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 2, 1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       1, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 3, 2, 2, 3, 3, 1,\n",
       "       3, 3, 3, 3, 2, 3, 3, 1, 3, 3, 3, 2, 3, 3, 3, 3, 1, 1, 1, 2, 3, 3,\n",
       "       3, 3, 3, 1, 1, 1, 3, 3, 3, 1, 3, 1, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3,\n",
       "       1, 3, 2, 1, 3, 3, 1, 1, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3,\n",
       "       3, 2, 3, 3, 3, 3, 1, 3, 3, 1, 3, 1, 1, 3, 3, 1, 3, 3, 3, 1, 3, 2,\n",
       "       3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 1, 3, 3, 3, 1, 3, 3, 3, 2, 2, 3, 3,\n",
       "       3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 2, 3, 3, 2, 3, 3, 1, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 1, 3, 3, 1, 1, 3, 1, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 1,\n",
       "       3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1,\n",
       "       3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3, 2, 1, 3, 1, 3, 1, 2,\n",
       "       3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 1, 3, 2, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 2, 3, 3, 3, 1, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3,\n",
       "       3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 1, 2, 2, 3,\n",
       "       3, 3, 3, 1, 3, 3, 2, 3, 2, 3, 1, 3, 3, 3, 3, 3, 3, 2, 3, 1, 2, 3,\n",
       "       3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 2, 3, 1, 2, 3, 1, 3, 1, 3, 3, 1,\n",
       "       3, 3, 3, 1, 3, 3, 1, 1, 3, 3, 3, 1, 3, 1, 3, 3, 1, 3, 3, 3, 3, 3,\n",
       "       1, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 1,\n",
       "       3, 3, 3, 3, 3, 2, 3, 3, 3, 1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C=1, Q=2, S=3, nan=S=3\n",
    "def embarked_to_integer(p):\n",
    "    if p == \"C\":\n",
    "        return 1\n",
    "    else :\n",
    "        if p == \"Q\":\n",
    "            return 2\n",
    "        else :\n",
    "            return 3\n",
    "k = embarked_to_integer\n",
    "x[\"Embarked\"] = np.array([k(i) for i in x[\"Embarked\"]])\n",
    "x[\"Embarked\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PassengerId\n",
      "Survived\n",
      "0\n",
      "Pclass\n",
      "0\n",
      "Name\n",
      "Sex\n",
      "0\n",
      "Age\n",
      "0\n",
      "SibSp\n",
      "0\n",
      "Parch\n",
      "0\n",
      "Ticket\n",
      "0\n",
      "Fare\n",
      "0\n",
      "Cabin\n",
      "687\n",
      "Embarked\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for cols in train.columns:\n",
    "    print(cols)\n",
    "    if str(cols)!=\"PassengerId\" and str(cols)!=\"Name\" and str(cols)!=\"\":        \n",
    "        print(str(check_nan2(x[str(cols)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok so cabin is usesless since it has too many nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['male', 'female', 'female', 'female', 'male', 'male', 'male',\n",
       "       'male', 'female', 'female', 'female', 'female', 'male', 'male',\n",
       "       'female', 'female', 'male', 'male', 'female', 'female', 'male',\n",
       "       'male', 'female', 'male', 'female', 'female', 'male', 'male',\n",
       "       'female', 'male', 'male', 'female', 'female', 'male', 'male',\n",
       "       'male', 'male', 'male', 'female', 'female', 'female', 'female',\n",
       "       'male', 'female', 'female', 'male', 'male', 'female', 'male',\n",
       "       'female', 'male', 'male', 'female', 'female', 'male', 'male',\n",
       "       'female', 'male', 'female', 'male', 'male', 'female', 'male',\n",
       "       'male', 'male', 'male', 'female', 'male', 'female', 'male', 'male',\n",
       "       'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male',\n",
       "       'female', 'male', 'male', 'female', 'male', 'female', 'female',\n",
       "       'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male',\n",
       "       'male', 'male', 'male', 'male', 'female', 'male', 'female', 'male',\n",
       "       'male', 'male', 'male', 'male', 'female', 'male', 'male', 'female',\n",
       "       'male', 'female', 'male', 'female', 'female', 'male', 'male',\n",
       "       'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male',\n",
       "       'male', 'male', 'male', 'female', 'male', 'male', 'male', 'female',\n",
       "       'female', 'male', 'male', 'female', 'male', 'male', 'male',\n",
       "       'female', 'female', 'female', 'male', 'male', 'male', 'male',\n",
       "       'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male',\n",
       "       'male', 'female', 'male', 'male', 'male', 'male', 'female', 'male',\n",
       "       'male', 'male', 'male', 'female', 'female', 'male', 'male', 'male',\n",
       "       'male', 'female', 'male', 'male', 'male', 'male', 'female', 'male',\n",
       "       'male', 'female', 'male', 'male', 'male', 'female', 'male',\n",
       "       'female', 'male', 'male', 'male', 'female', 'male', 'female',\n",
       "       'male', 'female', 'female', 'male', 'male', 'female', 'female',\n",
       "       'male', 'male', 'male', 'male', 'male', 'female', 'male', 'male',\n",
       "       'female', 'male', 'male', 'female', 'male', 'male', 'male',\n",
       "       'female', 'female', 'male', 'female', 'male', 'male', 'male',\n",
       "       'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female',\n",
       "       'female', 'male', 'male', 'female', 'male', 'female', 'male',\n",
       "       'female', 'male', 'male', 'female', 'female', 'male', 'male',\n",
       "       'male', 'male', 'female', 'female', 'male', 'male', 'male',\n",
       "       'female', 'male', 'male', 'female', 'female', 'female', 'female',\n",
       "       'female', 'female', 'male', 'male', 'male', 'male', 'female',\n",
       "       'male', 'male', 'male', 'female', 'female', 'male', 'male',\n",
       "       'female', 'male', 'female', 'female', 'female', 'male', 'male',\n",
       "       'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male',\n",
       "       'male', 'male', 'female', 'female', 'female', 'male', 'female',\n",
       "       'male', 'male', 'male', 'female', 'male', 'female', 'female',\n",
       "       'male', 'male', 'female', 'male', 'male', 'female', 'female',\n",
       "       'male', 'female', 'female', 'female', 'female', 'male', 'male',\n",
       "       'female', 'female', 'male', 'female', 'female', 'male', 'male',\n",
       "       'female', 'female', 'male', 'female', 'male', 'female', 'female',\n",
       "       'female', 'female', 'male', 'male', 'male', 'female', 'male',\n",
       "       'male', 'female', 'male', 'male', 'male', 'female', 'male', 'male',\n",
       "       'male', 'female', 'female', 'female', 'male', 'male', 'male',\n",
       "       'male', 'male', 'male', 'male', 'male', 'female', 'female',\n",
       "       'female', 'female', 'male', 'male', 'female', 'male', 'male',\n",
       "       'male', 'female', 'female', 'female', 'female', 'male', 'male',\n",
       "       'male', 'male', 'female', 'female', 'female', 'male', 'male',\n",
       "       'male', 'female', 'female', 'male', 'female', 'male', 'male',\n",
       "       'male', 'female', 'male', 'female', 'male', 'male', 'male',\n",
       "       'female', 'female', 'male', 'female', 'male', 'male', 'female',\n",
       "       'male', 'male', 'female', 'male', 'female', 'male', 'male', 'male',\n",
       "       'male', 'female', 'male', 'male', 'female', 'male', 'male',\n",
       "       'female', 'female', 'female', 'male', 'female', 'male', 'male',\n",
       "       'male', 'female', 'male', 'male', 'female', 'female', 'male',\n",
       "       'male', 'male', 'female', 'female', 'male', 'male', 'female',\n",
       "       'female', 'female', 'male', 'male', 'female', 'male', 'male',\n",
       "       'female', 'male', 'male', 'female', 'male', 'female', 'male',\n",
       "       'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female',\n",
       "       'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male',\n",
       "       'male', 'male', 'male', 'female', 'male', 'male', 'female',\n",
       "       'female', 'female', 'male', 'male', 'male', 'male', 'female',\n",
       "       'male', 'male', 'male', 'female', 'male', 'female', 'female',\n",
       "       'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male',\n",
       "       'male', 'female', 'male', 'female', 'male', 'male', 'female',\n",
       "       'female', 'female', 'female', 'male', 'female', 'male', 'male',\n",
       "       'male', 'male', 'male', 'male', 'female', 'male', 'male', 'female',\n",
       "       'male', 'female', 'male', 'female', 'male', 'male', 'female',\n",
       "       'male', 'male', 'female', 'male', 'male', 'male', 'female', 'male',\n",
       "       'male', 'female', 'female', 'female', 'male', 'female', 'male',\n",
       "       'female', 'female', 'female', 'female', 'male', 'male', 'male',\n",
       "       'female', 'male', 'male', 'male', 'male', 'male', 'male', 'male',\n",
       "       'female', 'male', 'female', 'male', 'female', 'female', 'male',\n",
       "       'male', 'male', 'male', 'female', 'male', 'male', 'female', 'male',\n",
       "       'male', 'male', 'female', 'male', 'female', 'male', 'male',\n",
       "       'female', 'female', 'female', 'male', 'female', 'female', 'male',\n",
       "       'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male',\n",
       "       'female', 'male', 'female', 'male', 'male', 'female', 'male',\n",
       "       'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male',\n",
       "       'male', 'male', 'female', 'female', 'female', 'male', 'female',\n",
       "       'male', 'male', 'female', 'male', 'female', 'female', 'male',\n",
       "       'male', 'male', 'male', 'male', 'male', 'male', 'male', 'female',\n",
       "       'male', 'male', 'male', 'male', 'male', 'male', 'female', 'female',\n",
       "       'male', 'male', 'female', 'male', 'male', 'female', 'female',\n",
       "       'male', 'female', 'male', 'male', 'male', 'male', 'female', 'male',\n",
       "       'female', 'male', 'female', 'female', 'male', 'male', 'female',\n",
       "       'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male',\n",
       "       'male', 'male', 'male', 'female', 'female', 'male', 'male', 'male',\n",
       "       'male', 'male', 'male', 'female', 'female', 'male', 'female',\n",
       "       'male', 'male', 'male', 'male', 'male', 'male', 'male', 'male',\n",
       "       'female', 'male', 'female', 'male', 'male', 'male', 'male', 'male',\n",
       "       'female', 'male', 'male', 'female', 'male', 'female', 'male',\n",
       "       'male', 'male', 'female', 'male', 'female', 'male', 'female',\n",
       "       'male', 'male', 'male', 'male', 'male', 'female', 'female', 'male',\n",
       "       'male', 'female', 'male', 'male', 'male', 'male', 'male', 'female',\n",
       "       'female', 'male', 'female', 'female', 'male', 'male', 'male',\n",
       "       'male', 'male', 'female', 'male', 'male', 'male', 'male', 'male',\n",
       "       'female', 'male', 'male', 'male', 'male', 'female', 'male', 'male',\n",
       "       'female', 'male', 'male', 'male', 'female', 'male', 'male', 'male',\n",
       "       'male', 'female', 'male', 'male', 'male', 'female', 'male',\n",
       "       'female', 'male', 'female', 'male', 'male', 'male', 'male',\n",
       "       'female', 'male', 'female', 'male', 'male', 'female', 'male',\n",
       "       'female', 'female', 'female', 'male', 'male', 'male', 'male',\n",
       "       'female', 'male', 'male', 'male', 'male', 'male', 'female', 'male',\n",
       "       'male', 'male', 'female', 'female', 'male', 'female', 'male',\n",
       "       'female', 'male', 'male', 'male', 'male', 'male', 'female', 'male',\n",
       "       'female', 'male', 'male', 'male', 'female', 'male', 'male',\n",
       "       'female', 'male', 'male', 'male', 'female', 'male', 'male',\n",
       "       'female', 'male', 'male', 'male', 'male', 'male', 'female',\n",
       "       'female', 'male', 'male', 'male', 'male', 'female', 'male', 'male',\n",
       "       'male', 'male', 'male', 'male', 'female', 'male', 'male', 'male',\n",
       "       'male', 'male', 'male', 'female', 'male', 'male', 'female',\n",
       "       'female', 'female', 'female', 'female', 'male', 'female', 'male',\n",
       "       'male', 'male', 'female', 'female', 'male', 'female', 'female',\n",
       "       'male', 'male', 'male', 'male', 'female', 'male', 'male', 'female',\n",
       "       'female', 'male', 'male', 'male', 'female', 'female', 'male',\n",
       "       'female', 'male', 'male', 'female', 'male', 'female', 'female',\n",
       "       'male', 'male'], dtype='<U6')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"Sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda a: 1 if a.lower() == \"male\" else 0\n",
    "#finding nan in \"Sex\"\n",
    "x[\"Sex\"] = np.array([f(i) for i in x[\"Sex\"]])#0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.  , 38.  , 26.  , 35.  , 35.  ,   nan, 54.  ,  2.  , 27.  ,\n",
       "       14.  ,  4.  , 58.  , 20.  , 39.  , 14.  , 55.  ,  2.  ,   nan,\n",
       "       31.  ,   nan, 35.  , 34.  , 15.  , 28.  ,  8.  , 38.  ,   nan,\n",
       "       19.  ,   nan,   nan, 40.  ,   nan,   nan, 66.  , 28.  , 42.  ,\n",
       "         nan, 21.  , 18.  , 14.  , 40.  , 27.  ,   nan,  3.  , 19.  ,\n",
       "         nan,   nan,   nan,   nan, 18.  ,  7.  , 21.  , 49.  , 29.  ,\n",
       "       65.  ,   nan, 21.  , 28.5 ,  5.  , 11.  , 22.  , 38.  , 45.  ,\n",
       "        4.  ,   nan,   nan, 29.  , 19.  , 17.  , 26.  , 32.  , 16.  ,\n",
       "       21.  , 26.  , 32.  , 25.  ,   nan,   nan,  0.83, 30.  , 22.  ,\n",
       "       29.  ,   nan, 28.  , 17.  , 33.  , 16.  ,   nan, 23.  , 24.  ,\n",
       "       29.  , 20.  , 46.  , 26.  , 59.  ,   nan, 71.  , 23.  , 34.  ,\n",
       "       34.  , 28.  ,   nan, 21.  , 33.  , 37.  , 28.  , 21.  ,   nan,\n",
       "       38.  ,   nan, 47.  , 14.5 , 22.  , 20.  , 17.  , 21.  , 70.5 ,\n",
       "       29.  , 24.  ,  2.  , 21.  ,   nan, 32.5 , 32.5 , 54.  , 12.  ,\n",
       "         nan, 24.  ,   nan, 45.  , 33.  , 20.  , 47.  , 29.  , 25.  ,\n",
       "       23.  , 19.  , 37.  , 16.  , 24.  ,   nan, 22.  , 24.  , 19.  ,\n",
       "       18.  , 19.  , 27.  ,  9.  , 36.5 , 42.  , 51.  , 22.  , 55.5 ,\n",
       "       40.5 ,   nan, 51.  , 16.  , 30.  ,   nan,   nan, 44.  , 40.  ,\n",
       "       26.  , 17.  ,  1.  ,  9.  ,   nan, 45.  ,   nan, 28.  , 61.  ,\n",
       "        4.  ,  1.  , 21.  , 56.  , 18.  ,   nan, 50.  , 30.  , 36.  ,\n",
       "         nan,   nan,  9.  ,  1.  ,  4.  ,   nan,   nan, 45.  , 40.  ,\n",
       "       36.  , 32.  , 19.  , 19.  ,  3.  , 44.  , 58.  ,   nan, 42.  ,\n",
       "         nan, 24.  , 28.  ,   nan, 34.  , 45.5 , 18.  ,  2.  , 32.  ,\n",
       "       26.  , 16.  , 40.  , 24.  , 35.  , 22.  , 30.  ,   nan, 31.  ,\n",
       "       27.  , 42.  , 32.  , 30.  , 16.  , 27.  , 51.  ,   nan, 38.  ,\n",
       "       22.  , 19.  , 20.5 , 18.  ,   nan, 35.  , 29.  , 59.  ,  5.  ,\n",
       "       24.  ,   nan, 44.  ,  8.  , 19.  , 33.  ,   nan,   nan, 29.  ,\n",
       "       22.  , 30.  , 44.  , 25.  , 24.  , 37.  , 54.  ,   nan, 29.  ,\n",
       "       62.  , 30.  , 41.  , 29.  ,   nan, 30.  , 35.  , 50.  ,   nan,\n",
       "        3.  , 52.  , 40.  ,   nan, 36.  , 16.  , 25.  , 58.  , 35.  ,\n",
       "         nan, 25.  , 41.  , 37.  ,   nan, 63.  , 45.  ,   nan,  7.  ,\n",
       "       35.  , 65.  , 28.  , 16.  , 19.  ,   nan, 33.  , 30.  , 22.  ,\n",
       "       42.  , 22.  , 26.  , 19.  , 36.  , 24.  , 24.  ,   nan, 23.5 ,\n",
       "        2.  ,   nan, 50.  ,   nan,   nan, 19.  ,   nan,   nan,  0.92,\n",
       "         nan, 17.  , 30.  , 30.  , 24.  , 18.  , 26.  , 28.  , 43.  ,\n",
       "       26.  , 24.  , 54.  , 31.  , 40.  , 22.  , 27.  , 30.  , 22.  ,\n",
       "         nan, 36.  , 61.  , 36.  , 31.  , 16.  ,   nan, 45.5 , 38.  ,\n",
       "       16.  ,   nan,   nan, 29.  , 41.  , 45.  , 45.  ,  2.  , 24.  ,\n",
       "       28.  , 25.  , 36.  , 24.  , 40.  ,   nan,  3.  , 42.  , 23.  ,\n",
       "         nan, 15.  , 25.  ,   nan, 28.  , 22.  , 38.  ,   nan,   nan,\n",
       "       40.  , 29.  , 45.  , 35.  ,   nan, 30.  , 60.  ,   nan,   nan,\n",
       "       24.  , 25.  , 18.  , 19.  , 22.  ,  3.  ,   nan, 22.  , 27.  ,\n",
       "       20.  , 19.  , 42.  ,  1.  , 32.  , 35.  ,   nan, 18.  ,  1.  ,\n",
       "       36.  ,   nan, 17.  , 36.  , 21.  , 28.  , 23.  , 24.  , 22.  ,\n",
       "       31.  , 46.  , 23.  , 28.  , 39.  , 26.  , 21.  , 28.  , 20.  ,\n",
       "       34.  , 51.  ,  3.  , 21.  ,   nan,   nan,   nan, 33.  ,   nan,\n",
       "       44.  ,   nan, 34.  , 18.  , 30.  , 10.  ,   nan, 21.  , 29.  ,\n",
       "       28.  , 18.  ,   nan, 28.  , 19.  ,   nan, 32.  , 28.  ,   nan,\n",
       "       42.  , 17.  , 50.  , 14.  , 21.  , 24.  , 64.  , 31.  , 45.  ,\n",
       "       20.  , 25.  , 28.  ,   nan,  4.  , 13.  , 34.  ,  5.  , 52.  ,\n",
       "       36.  ,   nan, 30.  , 49.  ,   nan, 29.  , 65.  ,   nan, 50.  ,\n",
       "         nan, 48.  , 34.  , 47.  , 48.  ,   nan, 38.  ,   nan, 56.  ,\n",
       "         nan,  0.75,   nan, 38.  , 33.  , 23.  , 22.  ,   nan, 34.  ,\n",
       "       29.  , 22.  ,  2.  ,  9.  ,   nan, 50.  , 63.  , 25.  ,   nan,\n",
       "       35.  , 58.  , 30.  ,  9.  ,   nan, 21.  , 55.  , 71.  , 21.  ,\n",
       "         nan, 54.  ,   nan, 25.  , 24.  , 17.  , 21.  ,   nan, 37.  ,\n",
       "       16.  , 18.  , 33.  ,   nan, 28.  , 26.  , 29.  ,   nan, 36.  ,\n",
       "       54.  , 24.  , 47.  , 34.  ,   nan, 36.  , 32.  , 30.  , 22.  ,\n",
       "         nan, 44.  ,   nan, 40.5 , 50.  ,   nan, 39.  , 23.  ,  2.  ,\n",
       "         nan, 17.  ,   nan, 30.  ,  7.  , 45.  , 30.  ,   nan, 22.  ,\n",
       "       36.  ,  9.  , 11.  , 32.  , 50.  , 64.  , 19.  ,   nan, 33.  ,\n",
       "        8.  , 17.  , 27.  ,   nan, 22.  , 22.  , 62.  , 48.  ,   nan,\n",
       "       39.  , 36.  ,   nan, 40.  , 28.  ,   nan,   nan, 24.  , 19.  ,\n",
       "       29.  ,   nan, 32.  , 62.  , 53.  , 36.  ,   nan, 16.  , 19.  ,\n",
       "       34.  , 39.  ,   nan, 32.  , 25.  , 39.  , 54.  , 36.  ,   nan,\n",
       "       18.  , 47.  , 60.  , 22.  ,   nan, 35.  , 52.  , 47.  ,   nan,\n",
       "       37.  , 36.  ,   nan, 49.  ,   nan, 49.  , 24.  ,   nan,   nan,\n",
       "       44.  , 35.  , 36.  , 30.  , 27.  , 22.  , 40.  , 39.  ,   nan,\n",
       "         nan,   nan, 35.  , 24.  , 34.  , 26.  ,  4.  , 26.  , 27.  ,\n",
       "       42.  , 20.  , 21.  , 21.  , 61.  , 57.  , 21.  , 26.  ,   nan,\n",
       "       80.  , 51.  , 32.  ,   nan,  9.  , 28.  , 32.  , 31.  , 41.  ,\n",
       "         nan, 20.  , 24.  ,  2.  ,   nan,  0.75, 48.  , 19.  , 56.  ,\n",
       "         nan, 23.  ,   nan, 18.  , 21.  ,   nan, 18.  , 24.  ,   nan,\n",
       "       32.  , 23.  , 58.  , 50.  , 40.  , 47.  , 36.  , 20.  , 32.  ,\n",
       "       25.  ,   nan, 43.  ,   nan, 40.  , 31.  , 70.  , 31.  ,   nan,\n",
       "       18.  , 24.5 , 18.  , 43.  , 36.  ,   nan, 27.  , 20.  , 14.  ,\n",
       "       60.  , 25.  , 14.  , 19.  , 18.  , 15.  , 31.  ,  4.  ,   nan,\n",
       "       25.  , 60.  , 52.  , 44.  ,   nan, 49.  , 42.  , 18.  , 35.  ,\n",
       "       18.  , 25.  , 26.  , 39.  , 45.  , 42.  , 22.  ,   nan, 24.  ,\n",
       "         nan, 48.  , 29.  , 52.  , 19.  , 38.  , 27.  ,   nan, 33.  ,\n",
       "        6.  , 17.  , 34.  , 50.  , 27.  , 20.  , 30.  ,   nan, 25.  ,\n",
       "       25.  , 29.  , 11.  ,   nan, 23.  , 23.  , 28.5 , 48.  , 35.  ,\n",
       "         nan,   nan,   nan, 36.  , 21.  , 24.  , 31.  , 70.  , 16.  ,\n",
       "       30.  , 19.  , 31.  ,  4.  ,  6.  , 33.  , 23.  , 48.  ,  0.67,\n",
       "       28.  , 18.  , 34.  , 33.  ,   nan, 41.  , 20.  , 36.  , 16.  ,\n",
       "       51.  ,   nan, 30.5 ,   nan, 32.  , 24.  , 48.  , 57.  ,   nan,\n",
       "       54.  , 18.  ,   nan,  5.  ,   nan, 43.  , 13.  , 17.  , 29.  ,\n",
       "         nan, 25.  , 25.  , 18.  ,  8.  ,  1.  , 46.  ,   nan, 16.  ,\n",
       "         nan,   nan, 25.  , 39.  , 49.  , 31.  , 30.  , 30.  , 34.  ,\n",
       "       31.  , 11.  ,  0.42, 27.  , 31.  , 39.  , 18.  , 39.  , 33.  ,\n",
       "       26.  , 39.  , 35.  ,  6.  , 30.5 ,   nan, 23.  , 31.  , 43.  ,\n",
       "       10.  , 52.  , 27.  , 38.  , 27.  ,  2.  ,   nan,   nan,  1.  ,\n",
       "         nan, 62.  , 15.  ,  0.83,   nan, 23.  , 18.  , 39.  , 21.  ,\n",
       "         nan, 32.  ,   nan, 20.  , 16.  , 30.  , 34.5 , 17.  , 42.  ,\n",
       "         nan, 35.  , 28.  ,   nan,  4.  , 74.  ,  9.  , 16.  , 44.  ,\n",
       "       18.  , 45.  , 51.  , 24.  ,   nan, 41.  , 21.  , 48.  ,   nan,\n",
       "       24.  , 42.  , 27.  , 31.  ,   nan,  4.  , 26.  , 47.  , 33.  ,\n",
       "       47.  , 28.  , 15.  , 20.  , 19.  ,   nan, 56.  , 25.  , 33.  ,\n",
       "       22.  , 28.  , 25.  , 39.  , 27.  , 19.  ,   nan, 26.  , 32.  ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"Age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##REFACTORING AGE:\n",
    "#finding the mean of age\n",
    "f = lambda a: a if not np.isnan(a) else 0\n",
    "\n",
    "age = np.array([f(u)for u in x[\"Age\"]])\n",
    "median_age = np.median(age)\n",
    "d = lambda a: a if not np.isnan(a) else median_age\n",
    "x[\"Age\"] = np.array([d(c) for c in x[\"Age\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80.0, 0.42)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"Age\"].max(), x[\"Age\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"Age\"] = x[\"Age\"]/x[\"Age\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0052499999999999995)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"Age\"].max(), x[\"Age\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 512.3292)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"Fare\"].min(), x[\"Fare\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, H3, H4):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, H3)\n",
    "        self.linear4 = nn.Linear(H3, H4) \n",
    "        self.linear5 = nn.Linear(H4, 1)# binary classifier\n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        x = torch.relu(self.linear3(x))\n",
    "        x = torch.relu(self.linear4(x))\n",
    "        x = torch.sigmoid(self.linear5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(7,10,8,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear1.weight',\n",
       "              tensor([[ 0.1961, -0.0172,  0.0515,  0.0866, -0.2647,  0.1222,  0.3150],\n",
       "                      [ 0.0365,  0.2812,  0.1434, -0.1223,  0.3308,  0.0058, -0.0490],\n",
       "                      [-0.3765,  0.2878,  0.2521, -0.3733,  0.1311, -0.0293, -0.2683],\n",
       "                      [-0.1298,  0.0282,  0.3733,  0.2833,  0.0170,  0.0881,  0.2216],\n",
       "                      [ 0.1958, -0.3434, -0.1090,  0.2302, -0.3220, -0.3596, -0.1063],\n",
       "                      [-0.0479,  0.0489, -0.3267, -0.2273, -0.3487,  0.0352,  0.1137],\n",
       "                      [ 0.2013, -0.2245,  0.0180,  0.0119, -0.1253,  0.1933,  0.0402],\n",
       "                      [-0.0823, -0.1929, -0.3124,  0.0316,  0.0643,  0.1672,  0.3335],\n",
       "                      [-0.1114, -0.2104, -0.1361, -0.2980,  0.3692,  0.2620, -0.2986],\n",
       "                      [ 0.3111, -0.2811,  0.3306, -0.0086, -0.2323, -0.2234,  0.2848]])),\n",
       "             ('linear1.bias',\n",
       "              tensor([ 0.0161,  0.1599, -0.1557,  0.2869, -0.1485,  0.1089,  0.3688,  0.3491,\n",
       "                      -0.3636, -0.0261])),\n",
       "             ('linear2.weight',\n",
       "              tensor([[-0.0076,  0.0700,  0.1798,  0.1648, -0.1110, -0.0436,  0.0872,  0.2235,\n",
       "                        0.1803, -0.2634],\n",
       "                      [ 0.0196, -0.0763,  0.0907, -0.2422, -0.1059, -0.1354,  0.0283, -0.1211,\n",
       "                       -0.0470,  0.0493],\n",
       "                      [-0.0651,  0.2923,  0.1399, -0.0232,  0.2686,  0.2964, -0.2063, -0.0728,\n",
       "                       -0.2976,  0.0288],\n",
       "                      [ 0.1982, -0.2726, -0.0546, -0.0318, -0.0071,  0.2874, -0.2919,  0.1476,\n",
       "                       -0.2266,  0.0033],\n",
       "                      [-0.1190, -0.2951,  0.2688, -0.0739, -0.3039, -0.0912,  0.0654, -0.0498,\n",
       "                        0.0547,  0.2156],\n",
       "                      [-0.2307, -0.2562,  0.1100, -0.0110, -0.2006,  0.1637,  0.2376,  0.2776,\n",
       "                        0.1765, -0.1821],\n",
       "                      [-0.1205,  0.1561, -0.2591, -0.2499, -0.3011,  0.2565,  0.1975,  0.0474,\n",
       "                        0.2632,  0.0069],\n",
       "                      [-0.1999, -0.2146, -0.1611,  0.2286, -0.0733,  0.0834, -0.0757, -0.2928,\n",
       "                       -0.1581,  0.0556]])),\n",
       "             ('linear2.bias',\n",
       "              tensor([ 0.1293, -0.0441, -0.1907,  0.1365, -0.0851,  0.1954, -0.0984,  0.2578])),\n",
       "             ('linear3.weight',\n",
       "              tensor([[ 0.0186, -0.0104, -0.1218,  0.1620, -0.0929,  0.3120,  0.1182,  0.1205],\n",
       "                      [-0.3308,  0.1006,  0.3285,  0.0297,  0.0212, -0.3471,  0.1068,  0.2003],\n",
       "                      [ 0.0087, -0.3055,  0.3045,  0.1019,  0.0069, -0.1388, -0.0853, -0.3034],\n",
       "                      [ 0.2470,  0.0358,  0.0994,  0.0768,  0.0121,  0.1385,  0.2405,  0.2590]])),\n",
       "             ('linear3.bias', tensor([ 0.2020,  0.3515, -0.1716, -0.3336])),\n",
       "             ('linear4.weight',\n",
       "              tensor([[ 0.2289, -0.3366, -0.0596,  0.4149],\n",
       "                      [-0.4868, -0.1957, -0.2241,  0.2680]])),\n",
       "             ('linear4.bias', tensor([0.1286, 0.4398])),\n",
       "             ('linear5.weight', tensor([[-0.5752,  0.1331]])),\n",
       "             ('linear5.bias', tensor([-0.3026]))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Optimizer.state_dict of Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0\n",
       ")>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 5],\n",
       "       [2, 6],\n",
       "       [3, 7],\n",
       "       [4, 8]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= np.array([[1],[2],[3],[4]])\n",
    "b = np.array([[5],[6],[7],[8]])\n",
    "np.concatenate((a, b), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([891, 7])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "X = []\n",
    "for col in train.columns:\n",
    "    logic =str(col)!=\"PassengerId\"and str(col)!=\"Ticket\" and str(col)!=\"Survived\"and str(col)!=\"Name\" and str(col)!=\"Cabin\"\n",
    "    if (not index) and logic:\n",
    "        #print(col)\n",
    "        X.append(x[str(col)])\n",
    "        X = np.array(X)\n",
    "        index+=1\n",
    "    else:\n",
    "        if logic:\n",
    "            X = np.concatenate((X, x[str(col)].reshape(1,x[str(col)].shape[0])), axis=0)\n",
    "            index+=1\n",
    "            \n",
    "X = X.T\n",
    "X = torch.Tensor(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.copy(x[\"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-51b85caa735e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m    925\u001b[0m     def __init__(self, weight=None, size_average=None, ignore_index=-100,\n\u001b[1;32m    926\u001b[0m                  reduce=None, reduction='mean'):\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_WeightedLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_WeightedLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py\u001b[0m in \u001b[0;36mlegacy_get_string\u001b[0;34m(size_average, reduce, emit_warning)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "y = model(X[1])\n",
    "loss = criterion(y, target)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y = model(X[1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
